---
sidebar: sidebar 
permalink: ai/aipod_nv_storage.html 
keywords: NetApp AI, AI, Artificial Intelligence, ML, Machine Learning, NVIDIA, NVIDIA AI Enterprise, NVIDIA BasePOD, NVIDIA DGX 
summary: NetApp AI Pod 搭配 NVIDIA DGX 系統 - 儲存系統設計與規模調整指南 
---
= NetApp AI Pod 搭配 NVIDIA DGX 系統 - 儲存系統設計與規模調整指南
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./../media/


link:aipod_nv_architecture.html["上一篇：採用 NVIDIA DGX 系統的 NetApp AI Pod - 架構。"]



== 儲存系統設計

每個 AFF A800 儲存系統都使用每個控制器的四個 100 GbE 連接埠進行連線。每個控制器的兩個連接埠用於從 DGX 系統存取工作負載資料、每個控制器的兩個連接埠則設定為 LACP 介面群組、以支援從管理平面伺服器存取叢集管理成品和使用者主目錄。儲存系統的所有資料存取都是透過 NFS 提供、其中儲存虛擬機器（ SVM ）專用於 AI 工作負載存取、另有專用於叢集管理用途的獨立 SVM 。

工作負載 SVM 總共設定了四個邏輯介面（生命）、每個儲存 VLAN 上有兩個生命。每個實體連接埠都會裝載兩個生命體、每個控制器上每個 VLAN 會產生兩個生命體。此組態可提供最大頻寬、以及將每個 LIF 容錯移轉到同一個控制器上的另一個連接埠的方法、以便在網路故障時、兩個控制器都能保持作用中。此組態也支援 NFS over RDMA 以啟用 GPUDirect 儲存存取。儲存容量是以橫跨兩個控制器的單一大型 FlexGroup 磁碟區的形式進行配置。此 FlexGroup 可從 SVM 上的任何生命負載存取、而 DGX A100 系統的裝載點則分散在可用的生命負載平衡中。

管理 SVM 只需要單一 LIF 、裝載在每個控制器上設定的雙連接埠介面群組上。管理 SVM 上的其他 FlexGroup 磁碟區則是用來存放叢集管理產出工件、例如叢集節點映像、系統監控歷史資料、以及終端使用者主目錄。下圖顯示儲存系統的邏輯組態。

image:oai_basepod1_logical.png["錯誤：缺少圖形影像"]



== 儲存系統規模調整指南

此架構是供想要在 NVIDIA DGX 系統和 NetApp AFF 儲存系統上實作 DL 基礎架構的客戶和合作夥伴參考。下表顯示每個 AFF 機型所支援的 A100 和 H100 GPU 數量的粗略估計值。

image:oai_sizing.png["錯誤：缺少圖形影像"]

如所示 link:https://www.netapp.com/pdf.html?item=/media/21793-nva-1153-design.pdf["此參考架構的舊版"]AFF A800 系統可輕鬆支援八個 DGX A100 系統所產生的 DL 訓練工作負載。上述其他儲存系統的預估值是根據這些結果計算、而 H100 GPU 的預估值則是 A100 系統所需的儲存處理量加倍。  對於儲存效能需求較高的大型部署、可在單一叢集中新增最多 12 個 HA 配對（ 24 個節點）的額外 AFF 系統至 NetApp ONTAP 叢集。使用本解決方案所述的 FlexGroup 技術、 24 節點叢集可在單一命名空間中提供 40 PB 以上的資料傳輸量、以及高達 300 Gbps 的傳輸量。其他 NetApp 儲存系統（例如 AFF A400 、 A250 和 C800 ）則提供較低的效能和 / 或較高的容量選項、以較低的成本進行較小型的部署。由於 ONTAP 9 支援混合模式叢集、因此客戶可以從較小的初始佔用空間開始、並在容量和效能需求增加時、將更多或更大的儲存系統新增至叢集。
link:aipod_nv_conclusion.html["下一步：採用 NVIDIA DGX 系統的 NetApp AI Pod - 結論。"]
