---
sidebar: sidebar 
permalink: ai/ai-edge-test-procedure.html 
keywords: procedure, operating system, ubuntu, nvidia, docker, criteo, brats 
summary: 本節說明驗證此解決方案所使用的測試程序。 
---
= 測試程序
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./../media/


link:ai-edge-test-configuration.html["上一步：測試組態。"]

[role="lead"]
我們在此驗證中使用下列測試程序。



== 作業系統與AI推斷設定

在支援NVIDIA GPU的情況下、我們使用Ubuntu 18.04搭配NVIDIA驅動程式和Docker、並使用MLPerf AFF https://github.com/mlperf/inference_results_v0.7/tree/master/closed/Lenovo["程式碼"^] 在Lenovo提交給MLPerf Inference v0.7的過程中提供。

對於EF280、我們使用Ubuntu 20.04搭配NVIDIA驅動程式和Docker、支援NVIDIA GPU和MLPerf https://github.com/mlcommons/inference_results_v1.1/tree/main/closed/Lenovo["程式碼"^] 在Lenovo提交給MLPerf Inference v1.1的過程中提供。

若要設定AI推斷、請遵循下列步驟：

. 下載需要登錄的資料集、ImageNet 2012驗證集、Criteo TB資料集、以及Brat 2019訓練集、然後解壓縮檔案。
. 建立至少1TB的工作目錄、並定義環境變數「MarlFF_Scrate_path」（指目錄）。
+
您應該在共用儲存設備上共用此目錄、以供網路儲存設備使用、或在測試本機資料時共用本機磁碟。

. 執行make「prebuild」命令、此命令會建立及啟動泊塢視窗容器、以執行必要的推斷工作。
+

NOTE: 下列命令都是從執行中的Docker容器中執行：

+
** 下載預先訓練的AI模型以執行MLPerf推斷工作：「make download_model」
** 下載其他可免費下載的資料集：「make download_data」
** 預先處理資料：製作「preprocure_data」
** RUN：「make build」。
** 在運算伺服器中建置專為GPU最佳化的推斷引擎：「make general_引擎」
** 若要執行推斷工作負載、請執行下列（一個命令）：




....
make run_harness RUN_ARGS="--benchmarks=<BENCHMARKS> --scenarios=<SCENARIOS>"
....


== AI推斷會執行

執行了三種類型的執行：

* 使用本機儲存設備的單一伺服器AI推斷
* 使用網路儲存設備的單一伺服器AI推斷
* 使用網路儲存設備的多伺服器AI推斷


link:ai-edge-test-results.html["下一步：測試結果。"]
