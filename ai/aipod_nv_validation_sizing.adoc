---
sidebar: sidebar 
permalink: ai/aipod_nv_validation_sizing.html 
keywords: NetApp AI, AI, Artificial Intelligence, ML, Machine Learning, NVIDIA, NVIDIA AI Enterprise, NVIDIA BasePOD, NVIDIA DGX 
summary: NetApp AIPod 搭配 NVIDIA DGX 系統 - 結論 
---
= NetApp AIPod 搭配 NVIDIA DGX 系統 - 解決方案驗證與規模調整指南
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./../media/


link:aipod_nv_architecture.html["上一篇：採用 NVIDIA DGX 系統的 NetApp AIPod - 解決方案架構"]



== 解決方案驗證

此解決方案中的儲存組態已使用一系列使用開放原始碼工具 FIO 的綜合工作負載進行驗證。這些測試包括讀寫 I/O 模式、用於模擬 DGX 系統執行深度學習訓練工作所產生的儲存工作負載。儲存組態已通過驗證、使用雙插槽 CPU 伺服器叢集同時執行 FIO 工作負載、以模擬 DGX 系統叢集。每個用戶端都設定了先前所述的相同網路組態、並加入下列詳細資料。

下列掛載選項用於此驗證：
• ves=4.1 # 可讓 pNFS 平行存取多個儲存節點
• proto=RDMA # 將傳輸通訊協定設定為 RDMA 、而非預設 TCP
•連接埠 = 20049 # 指定 RDMA NFS 服務的正確連接埠
• max_connect = 16 # 可讓 NFS 工作階段主幹來集合儲存連接埠頻寬
•寫入 = 渴望 # 改善緩衝寫入的寫入效能
• rsize=262144 、 wsize=262144 # 將 I/O 傳輸大小設為 256k

此外、用戶端設定的 NFS max_Session_插 槽值為 1024 。在透過 RDMA 使用 NFS 測試解決方案時、儲存網路連接埠已設定為主動 / 被動連結。下列債券參數用於此驗證 -
•模式 = 主動式備份 # 將連結設定為主動 / 被動模式
• <interface name> 所有用戶端的 # 個主要介面都分散在交換器上
• MII-monitor-interval = 100 # 指定 100ms 的監控時間間隔
•容錯移轉 -Mac-policy=active # 指定主動鏈的 MAC 地址是綁定的 MAC 地址。這是在連結介面上正確操作 RDMA 所需的。

儲存系統的設定方式如前所述、為兩對 A900 HA （ 4 個控制器）搭配兩個 NS224 磁碟櫃（每對 HA 連接 24 個 1.9TB NVMe 磁碟機）。如架構一節所述、所有控制器的儲存容量都是使用 FlexGroup 磁碟區進行組合、而來自所有用戶端的資料則分散在叢集中的所有控制器上。



== 儲存系統規模調整指南

根據上述測試、 NVDIIA 已證實所測試的儲存組態可輕鬆支援八個 DGX H100 系統的叢集。對於儲存效能需求較高的大型部署、可在單一叢集中新增最多 12 個 HA 配對（ 24 個節點）的額外 AFF 系統至 NetApp ONTAP 叢集。使用本解決方案所述的 FlexGroup 技術、 24 節點叢集可在單一命名空間中提供 40 PB 以上的資料傳輸量、以及高達 300 Gbps 的傳輸量。其他 NetApp 儲存系統（例如 AFF A400 、 A250 和 C800 ）則提供較低的效能和 / 或較高的容量選項、以較低的成本進行較小型的部署。由於 ONTAP 9 支援混合模式叢集、因此客戶可以從較小的初始佔用空間開始、並在容量和效能需求增加時、將更多或更大的儲存系統新增至叢集。下表顯示每個 AFF 機型所支援的 A100 和 H100 GPU 數量的粗略估計值。

image:aipod_nv_sizing.png["錯誤：缺少圖形影像"]

link:aipod_nv_conclusion_add_info.html["下一步：採用 NVIDIA DGX 系統的 NetApp AIPod - 結論與其他資訊"]
