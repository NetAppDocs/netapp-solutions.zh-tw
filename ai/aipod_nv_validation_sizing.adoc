---
sidebar: sidebar 
permalink: ai/aipod_nv_validation_sizing.html 
keywords: NetApp AI, AI, Artificial Intelligence, ML, Machine Learning, NVIDIA, NVIDIA AI Enterprise, NVIDIA BasePOD, NVIDIA DGX 
summary: NetApp AIPod 搭配 NVIDIA DGX 系統 - 解決方案驗證與規模調整指南 
---
= NVA-1173 NetApp AIPod 搭配 NVIDIA DGX 系統 - 解決方案驗證與規模調整指南
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ../media/


[role="lead"]
本節著重於 NetApp AIPod 搭配 NVIDIA DGX 系統的解決方案驗證與規模調整指南。



== 解決方案驗證

此解決方案中的儲存組態已使用一系列使用開放原始碼工具 FIO 的綜合工作負載進行驗證。這些測試包括讀寫 I/O 模式、用於模擬 DGX 系統執行深度學習訓練工作所產生的儲存工作負載。儲存組態已通過驗證、使用雙插槽 CPU 伺服器叢集同時執行 FIO 工作負載、以模擬 DGX 系統叢集。每個用戶端都設定了先前所述的相同網路組態、並加入下列詳細資料。

此驗證使用下列掛載選項：

[cols="30%, 70%"]
|===


| ves=4.1 | 啟用 pNFS 以平行存取多個儲存節點 


| proto=RDMA | 將傳輸通訊協定設定為 RDMA 、而非預設 TCP 


| 連接埠 = 20049 | 為 RDMA NFS 服務指定正確的連接埠 


| max_connect = 16 | 啟用 NFS 工作階段主幹以彙總儲存連接埠頻寬 


| 寫入 = 渴望 | 改善緩衝寫入的寫入效能 


| rsize=262144、wsize=262144 | 將 I/O 傳輸大小設為 256k 
|===
此外、用戶端設定的 NFS max_Session_插 槽值為 1024 。在透過 RDMA 使用 NFS 測試解決方案時、儲存網路連接埠已設定為主動 / 被動連結。此驗證使用下列連結參數：

[cols="30%, 70%"]
|===


| mode=active-backup | 將連結設定為主動 / 被動模式 


| primary = <interface name> | 所有用戶端的主要介面都分散在交換器上 


| MII-monitor-interval = 100 | 指定 100ms 的監控時間間隔 


| 容錯移轉 -Mac-policy=active | 指定主動鏈的 MAC 地址是綁定的 MAC 地址。這是在連結介面上正確操作 RDMA 所需的。 
|===
儲存系統的設定方式如前所述、為兩對 A900 HA （ 4 個控制器）搭配兩個 NS224 磁碟櫃（每對 HA 連接 24 個 1.9TB NVMe 磁碟機）。如架構一節所述、所有控制器的儲存容量都是使用 FlexGroup 磁碟區進行組合、而來自所有用戶端的資料則分散在叢集中的所有控制器上。



== 儲存系統規模調整指南

NetApp 已成功完成 DGX BasePOD 認證、兩對通過測試的 A90 HA 可輕鬆支援 16 個 DGX H100 系統的叢集。對於儲存效能需求較高的大型部署、可在單一叢集中新增最多 12 個 HA 配對（ 24 個節點）的額外 AFF 系統至 NetApp ONTAP 叢集。使用本解決方案所述的 FlexGroup 技術、 24 節點叢集可在單一命名空間中提供 40 PB 以上的資料傳輸量、以及高達 300 Gbps 的傳輸量。其他 NetApp 儲存系統（例如 AFF A400 、 A250 和 C800 ）則提供較低的效能和 / 或較高的容量選項、以較低的成本進行較小型的部署。由於 ONTAP 9 支援混合模式叢集、因此客戶可以從較小的初始佔用空間開始、並在容量和效能需求增加時、將更多或更大的儲存系統新增至叢集。下表顯示每個 AFF 機型所支援的 A100 和 H100 GPU 數量的粗略估計值。

NetApp 儲存系統規模調整指南 _

image:aipod_nv_sizing_new.png["此圖顯示輸入 / 輸出對話方塊或表示寫入內容"]
