---
sidebar: sidebar 
permalink: data-analytics/kafka-nfs-performance-overview-and-validation-in-aws.html 
keywords: AWS cloud, ha pair, high availability, openmessage benchmarking, architectural setup 
summary: 安裝在NetApp NFS上的儲存層Kafka叢集、是以AWS雲端效能為基準。基準測試範例將於下列各節中說明。 
---
= AWS的效能總覽與驗證
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./../media/


link:kafka-nfs-why-netapp-nfs-for-kafka-workloads.html["上一篇：為什麼要使用NetApp NFS來處理Kafka工作負載？"]

[role="lead"]
安裝在NetApp NFS上的儲存層Kafka叢集、是以AWS雲端效能為基準。基準測試範例將於下列各節中說明。



== 採用NetApp Cloud Volumes ONTAP 技術的AWS雲端中的Kafka（高可用度配對與單一節點）

採用NetApp Cloud Volumes ONTAP 功能的Kafka叢集（HA配對）是AWS雲端效能的基準測試。以下各節將說明此基準測試。



=== 架構設定

下表顯示使用NAS的Kafka叢集環境組態。

|===
| 平台元件 | 環境組態 


| Kafka 3.2.3  a| 
* 3個zookeepers–T2.small
* 3個代理伺服器–i3en.2xLarge
* 1 x Grafana–c5n.2xLarge
* 4個製造商/消費者- c5n.2xLarge *




| 所有節點上的作業系統 | RHEL8.6. 


| NetApp Cloud Volumes ONTAP 執行個體 | HA配對執行個體–m5dn.12xLargex 2節點單一節點執行個體- m5dn.12xLargex 1節點 
|===


=== NetApp叢集Volume ONTAP 不全

. 針對「樣片HA配對」、我們在每個儲存控制器的每個集合體上建立了兩個集合體、並有三個磁碟區Cloud Volumes ONTAP 。對於單Cloud Volumes ONTAP 一的實體節點、我們會在一個集合體中建立六個磁碟區。
+
image:kafka-nfs-image25.png["此影像描述aggr3和aggr22的屬性。"]

+
image:kafka-nfs-image26.png["此影像描述aggr2的屬性。"]

. 為了提升網路效能、我們同時為HA配對和單一節點啟用高速網路功能。
+
image:kafka-nfs-image27.png["此影像說明如何啟用高速網路。"]

. 我們注意到ONTAP 、由於不支援IOPS、Cloud Volumes ONTAP 所以我們將IOPS變更為2350、以供支援整個核心磁碟區使用。以47GB大小為基礎的根Volume磁碟Cloud Volumes ONTAP 。下列ONTAP 支援HA配對的支援功能使用下列支援功能、單一節點也適用相同的步驟。
+
....
statistics start -object vnvram -instance vnvram -counter backing_store_iops -sample-id sample_555
kafka_nfs_cvo_ha1::*> statistics show -sample-id sample_555
Object: vnvram
Instance: vnvram
Start-time: 1/18/2023 18:03:11
End-time: 1/18/2023 18:03:13
Elapsed-time: 2s
Scope: kafka_nfs_cvo_ha1-01
    Counter                                                     Value
    -------------------------------- --------------------------------
    backing_store_iops                                           1479
Object: vnvram
Instance: vnvram
Start-time: 1/18/2023 18:03:11
End-time: 1/18/2023 18:03:13
Elapsed-time: 2s
Scope: kafka_nfs_cvo_ha1-02
    Counter                                                     Value
    -------------------------------- --------------------------------
    backing_store_iops                                           1210
2 entries were displayed.
kafka_nfs_cvo_ha1::*>
....
+
image:kafka-nfs-image28.png["此影像說明如何修改Volume內容。"]



下圖說明NAS型Kafka叢集的架構。

* *運算*我們使用三節點的Kafka叢集、其中三節點的zookeeper集合體執行於專屬的伺服器上。每個代理商都有兩個NFS掛載點、Cloud Volumes ONTAP 透過專屬的LIF、連接到位於整個過程的單一Volume。
* *監控*我們使用兩個節點來搭配Prometheus-Grafana。為了產生工作負載、我們使用了一個獨立的三節點叢集、可以產生和使用這個Kafka叢集。
* * Storage。* Cloud Volumes ONTAP 我們使用HA配對的不二執行個體、在執行個體上掛載一個6TB GP3 AWS-EBS磁碟區。然後使用NFS掛載將磁碟區匯出至Kafka代理程式。


image:kafka-nfs-image29.png["此圖說明以NAS為基礎的Kafka叢集架構。"]



=== OpenMessage基準測試組態

. 為了提升NFS效能、我們需要在NFS伺服器和NFS用戶端之間建立更多的網路連線、而NFS用戶端可以使用nconnect建立。執行下列命令、以nconnect選項在代理節點上掛載NFS磁碟區：
+
....
[root@ip-172-30-0-121 ~]# cat /etc/fstab
UUID=eaa1f38e-de0f-4ed5-a5b5-2fa9db43bb38/xfsdefaults00
/dev/nvme1n1 /mnt/data-1 xfs defaults,noatime,nodiscard 0 0
/dev/nvme2n1 /mnt/data-2 xfs defaults,noatime,nodiscard 0 0
172.30.0.233:/kafka_aggr3_vol1 /kafka_aggr3_vol1 nfs defaults,nconnect=16 0 0
172.30.0.233:/kafka_aggr3_vol2 /kafka_aggr3_vol2 nfs defaults,nconnect=16 0 0
172.30.0.233:/kafka_aggr3_vol3 /kafka_aggr3_vol3 nfs defaults,nconnect=16 0 0
172.30.0.242:/kafka_aggr22_vol1 /kafka_aggr22_vol1 nfs defaults,nconnect=16 0 0
172.30.0.242:/kafka_aggr22_vol2 /kafka_aggr22_vol2 nfs defaults,nconnect=16 0 0
172.30.0.242:/kafka_aggr22_vol3 /kafka_aggr22_vol3 nfs defaults,nconnect=16 0 0
[root@ip-172-30-0-121 ~]# mount -a
[root@ip-172-30-0-121 ~]# df -h
Filesystem                       Size  Used Avail Use% Mounted on
devtmpfs                          31G     0   31G   0% /dev
tmpfs                             31G  249M   31G   1% /run
tmpfs                             31G     0   31G   0% /sys/fs/cgroup
/dev/nvme0n1p2                    10G  2.8G  7.2G  28% /
/dev/nvme1n1                     2.3T  248G  2.1T  11% /mnt/data-1
/dev/nvme2n1                     2.3T  245G  2.1T  11% /mnt/data-2
172.30.0.233:/kafka_aggr3_vol1   1.0T   12G 1013G   2% /kafka_aggr3_vol1
172.30.0.233:/kafka_aggr3_vol2   1.0T  5.5G 1019G   1% /kafka_aggr3_vol2
172.30.0.233:/kafka_aggr3_vol3   1.0T  8.9G 1016G   1% /kafka_aggr3_vol3
172.30.0.242:/kafka_aggr22_vol1  1.0T  7.3G 1017G   1% /kafka_aggr22_vol1
172.30.0.242:/kafka_aggr22_vol2  1.0T  6.9G 1018G   1% /kafka_aggr22_vol2
172.30.0.242:/kafka_aggr22_vol3  1.0T  5.9G 1019G   1% /kafka_aggr22_vol3
tmpfs                            6.2G     0  6.2G   0% /run/user/1000
[root@ip-172-30-0-121 ~]#
....
. 檢查Cloud Volumes ONTAP 內部的網路連線。下列ONTAP 支援從單Cloud Volumes ONTAP 一支援節點使用下列支援功能。同樣的步驟也適用於Cloud Volumes ONTAP 此功能。
+
....
Last login time: 1/20/2023 00:16:29
kafka_nfs_cvo_sn::> network connections active show -service nfs* -fields remote-host
node                cid        vserver              remote-host
------------------- ---------- -------------------- ------------
kafka_nfs_cvo_sn-01 2315762628 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762629 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762630 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762631 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762632 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762633 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762634 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762635 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762636 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762637 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762639 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762640 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762641 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762642 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762643 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762644 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762645 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762646 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762647 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762648 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762649 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762650 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762651 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762652 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762653 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762656 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762657 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762658 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762659 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762660 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762661 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762662 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762663 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762664 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762665 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762666 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762667 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762668 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762669 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762670 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762671 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762672 svm_kafka_nfs_cvo_sn 172.30.0.72
kafka_nfs_cvo_sn-01 2315762673 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762674 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762676 svm_kafka_nfs_cvo_sn 172.30.0.121
kafka_nfs_cvo_sn-01 2315762677 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762678 svm_kafka_nfs_cvo_sn 172.30.0.223
kafka_nfs_cvo_sn-01 2315762679 svm_kafka_nfs_cvo_sn 172.30.0.223
48 entries were displayed.
 
kafka_nfs_cvo_sn::>
....
. 我們使用下列Kafka `server.properties` 所有的Kafka經紀人都能提供Cloud Volumes ONTAP。 `log.dirs` 每個代理的屬性各不相同、其餘屬性則適用於代理程式。若為Broker1、則為 `log.dirs` 價值如下：
+
....
[root@ip-172-30-0-121 ~]# cat /opt/kafka/config/server.properties
broker.id=0
advertised.listeners=PLAINTEXT://172.30.0.121:9092
#log.dirs=/mnt/data-1/d1,/mnt/data-1/d2,/mnt/data-1/d3,/mnt/data-2/d1,/mnt/data-2/d2,/mnt/data-2/d3
log.dirs=/kafka_aggr3_vol1/broker1,/kafka_aggr3_vol2/broker1,/kafka_aggr3_vol3/broker1,/kafka_aggr22_vol1/broker1,/kafka_aggr22_vol2/broker1,/kafka_aggr22_vol3/broker1
zookeeper.connect=172.30.0.12:2181,172.30.0.30:2181,172.30.0.178:2181
num.network.threads=64
num.io.threads=64
socket.send.buffer.bytes=102400
socket.receive.buffer.bytes=102400
socket.request.max.bytes=104857600
num.partitions=1
num.recovery.threads.per.data.dir=1
offsets.topic.replication.factor=1
transaction.state.log.replication.factor=1
transaction.state.log.min.isr=1
replica.fetch.max.bytes=524288000
background.threads=20
num.replica.alter.log.dirs.threads=40
num.replica.fetchers=20
[root@ip-172-30-0-121 ~]#
....
+
** 若為Broker2 `log.dirs` 屬性值如下：
+
....
log.dirs=/kafka_aggr3_vol1/broker2,/kafka_aggr3_vol2/broker2,/kafka_aggr3_vol3/broker2,/kafka_aggr22_vol1/broker2,/kafka_aggr22_vol2/broker2,/kafka_aggr22_vol3/broker2
....
** 若為Broker3 `log.dirs` 屬性值如下：
+
....
log.dirs=/kafka_aggr3_vol1/broker3,/kafka_aggr3_vol2/broker3,/kafka_aggr3_vol3/broker3,/kafka_aggr22_vol1/broker3,/kafka_aggr22_vol2/broker3,/kafka_aggr22_vol3/broker3
....


. 對於單Cloud Volumes ONTAP 一的支援節點、卡夫卡（Kafka） `servers.properties` 與Cloud Volumes ONTAP 不包括在內的其他不相同 `log.dirs` 屬性。
+
** 若為Broker1、則為 `log.dirs` 價值如下：
+
....
log.dirs=/kafka_aggr2_vol1/broker1,/kafka_aggr2_vol2/broker1,/kafka_aggr2_vol3/broker1,/kafka_aggr2_vol4/broker1,/kafka_aggr2_vol5/broker1,/kafka_aggr2_vol6/broker1
....
** 若為Broker2 `log.dirs` 價值如下：
+
....
log.dirs=/kafka_aggr2_vol1/broker2,/kafka_aggr2_vol2/broker2,/kafka_aggr2_vol3/broker2,/kafka_aggr2_vol4/broker2,/kafka_aggr2_vol5/broker2,/kafka_aggr2_vol6/broker2
....
** 若為Broker3 `log.dirs` 屬性值如下：
+
....
log.dirs=/kafka_aggr2_vol1/broker3,/kafka_aggr2_vol2/broker3,/kafka_aggr2_vol3/broker3,/kafka_aggr2_vol4/broker3,/kafka_aggr2_vol5/broker3,/kafka_aggr2_vol6/broker3
....


. OMB中的工作負載會設定下列內容： `(/opt/benchmark/workloads/1-topic-100-partitions-1kb.yaml)`。
+
....
topics: 4
partitionsPerTopic: 100
messageSize: 32768
useRandomizedPayloads: true
randomBytesRatio: 0.5
randomizedPayloadPoolSize: 100
subscriptionsPerTopic: 1
consumerPerSubscription: 80
producersPerTopic: 40
producerRate: 1000000
consumerBacklogSizeGB: 0
testDurationMinutes: 5
....
+
。 `messageSize` 可能因使用案例而異。在效能測試中、我們使用3K。

+
我們使用OMB的兩個不同驅動程式：同步或處理量、來產生Kafka叢集上的工作負載。

+
** 用於Sync驅動程式內容的yaml檔案如下 `(/opt/benchmark/driver- kafka/kafka-sync.yaml)`：
+
....
name: Kafka
driverClass: io.openmessaging.benchmark.driver.kafka.KafkaBenchmarkDriver
# Kafka client-specific configuration
replicationFactor: 3
topicConfig: |
  min.insync.replicas=2
  flush.messages=1
  flush.ms=0
commonConfig: |
  bootstrap.servers=172.30.0.121:9092,172.30.0.72:9092,172.30.0.223:9092
producerConfig: |
  acks=all
  linger.ms=1
  batch.size=1048576
consumerConfig: |
  auto.offset.reset=earliest
  enable.auto.commit=false
  max.partition.fetch.bytes=10485760
....
** 用於處理量驅動程式內容的yaml檔案如下 `(/opt/benchmark/driver- kafka/kafka-throughput.yaml)`：
+
....
name: Kafka
driverClass: io.openmessaging.benchmark.driver.kafka.KafkaBenchmarkDriver
# Kafka client-specific configuration
replicationFactor: 3
topicConfig: |
  min.insync.replicas=2
commonConfig: |
  bootstrap.servers=172.30.0.121:9092,172.30.0.72:9092,172.30.0.223:9092
  default.api.timeout.ms=1200000
  request.timeout.ms=1200000
producerConfig: |
  acks=all
  linger.ms=1
  batch.size=1048576
consumerConfig: |
  auto.offset.reset=earliest
  enable.auto.commit=false
  max.partition.fetch.bytes=10485760
....






== 測試方法

. 根據上述規格、我們使用Terraform和Ansible來配置Kafka叢集。Terraform是用來為Kafka叢集使用AWS執行個體來建置基礎架構、Ansible則是在這些執行個體上建置Kafka叢集。
. 使用上述工作負載組態和Sync驅動程式觸發OMB工作負載。
+
....
Sudo bin/benchmark –drivers driver-kafka/kafka- sync.yaml workloads/1-topic-100-partitions-1kb.yaml
....
. 另一個工作負載是透過具有相同工作負載組態的處理量驅動程式觸發。
+
....
sudo bin/benchmark –drivers driver-kafka/kafka-throughput.yaml workloads/1-topic-100-partitions-1kb.yaml
....




== 觀察

使用兩種不同類型的驅動程式來產生工作負載、以基準測試在NFS上執行的Kafka執行個體效能。驅動程式之間的差異在於記錄排清內容。

若為Cloud Volumes ONTAP 「解決方案」配對：

* Sync驅動程式持續產生的總處理量：約1236 Mbps。
* 為處理量驅動程式產生的總處理量：尖峰約1412 Mbps。


對於單Cloud Volumes ONTAP 一的節點：

* Sync驅動程式持續產生的總處理量：約1962MBps。
* 處理量驅動程式產生的總處理量：尖峰約1660MBps


同步處理驅動程式可在記錄立即排入磁碟時產生一致的處理量、而處理量驅動程式則會在大量將記錄提交至磁碟時產生大量處理量。

這些處理量編號是針對指定的AWS組態所產生。為了達到更高的效能需求、可以進一步擴充和調整執行個體類型、以獲得更好的處理量。總處理量或總處理率是生產者和使用者速率的組合。

image:kafka-nfs-image30.png["此處顯示四種不同的圖表。CVO-HA配對處理量驅動程式。CVO-HA配對同步驅動程式。CVO單一節點處理量驅動程式。CVO單節點同步驅動程式。"]

執行處理量或同步驅動程式基準測試時、請務必檢查儲存處理量。

image:kafka-nfs-image31.png["此圖表顯示延遲、IOPS和處理量的效能。"]



== AWS FSxN 中的 Apache Kafka



=== 總覽

網路檔案系統（ NFS ）是廣泛使用的網路檔案系統、用於儲存大量資料。在大多數組織中、像 Apache Kafka 這樣的串流應用程式越來越多地產生資料。這些工作負載需要擴充性、低延遲、以及具備現代化儲存功能的健全資料擷取架構。為了實現即時分析並提供可據以行動的洞見、需要設計完善且效能優異的基礎架構。

Kafka by 的設計可搭配 POSIX 相容的檔案系統使用、並仰賴檔案系統來處理檔案作業、但在 NFSv3 檔案系統上儲存資料時、 Kafka Broker NFS 用戶端可以與本地檔案系統（例如 XFS 或 ext4 ）不同地解譯檔案作業。常見的例子是 NFS 愚蠢重新命名、導致 Kafka Brokers 在擴充叢集和重新分配分割區時失敗。為了因應這項挑戰、 NetApp 已更新開放原始碼 Linux NFS 用戶端、現在已在 RHEL8.7 、 RHEL9.1 中普遍提供變更、並從目前的適用於 ONTAP 版本 ONTAP 9.12.1 的 FSX 中獲得支援。

Amazon FSX for NetApp ONTAP 可在雲端中提供完全託管、可擴充且高效能的 NFS 檔案系統。適用於 NetApp 的 FSX 上的 Kafka 資料可擴充以處理大量資料、並確保容錯能力。NFS 可為重要且敏感的資料集提供集中化的儲存管理與資料保護。

這些增強功能可讓 AWS 客戶在 AWS 運算服務上執行 Kafka 工作負載時、善用適用於 ONTAP 的 FSX 。這些效益包括：
* 降低 CPU 使用率、以縮短 I/O 等待時間
* 更快的 Kafka 代理程式恢復時間
* 可靠性與效率
* 擴充性與效能
* 多可用性區域可用度
* 資料保護



=== AWS FSxN 中的效能概觀與驗證

NetApp NFS 上安裝儲存層的 Kafka 叢集、是 AWS FSxN 效能的基準測試。基準測試範例將於下列各節中說明。



==== AWS FSxN 中的 Kafka （主動式被動式）

採用 AWS FSxN 的 Kafka 叢集、是 AWS 雲端效能的基準測試結果。以下各節將說明此基準測試。



==== 架構設定

下表顯示使用 AWS FSxN 的 Kafka 叢集環境組態。

|===
| 平台元件 | 環境組態 


| Kafka 3.2.3  a| 
* 3個zookeepers–T2.small
* 3個代理伺服器–i3en.2xLarge
* 1 x Grafana–c5n.2xLarge
* 4個製造商/消費者- c5n.2xLarge *




| 所有節點上的作業系統 | RHEL8.6. 


| AWS FSxN | 主動被動執行個體、處理量為 4GB/ 秒、 IPS 為 160000 
|===


==== NetApp FSxN 設定

. 在我們的初始測試中、我們為 NetApp ONTAP 檔案系統建立了 FSX 、其 2 TB 和 4 、 000 IOPs 的處理量為 2 GB/ 秒。
. 在 FSX for NetApp ONTAP 中、我們測試區域（美國東部 -1 ）每秒 2 GB 處理量檔案系統的最高 IOPS 為 8 、 000 IOPS 。NetApp ONTAP 檔案系統的 FSX 總 IOPS 為 16 、 000 IOPS 、需要每秒 4 GB 的處理量部署、我們將在本文件稍後說明
+
....
[root@ip-172-31-33-69 ~]# aws fsx create-file-system --region us-east-2  --storage-capacity 2048 --subnet-ids <desired subnet 1> subnet-<desired subnet 2> --file-system-type ONTAP --ontap-configuration DeploymentType=MULTI_AZ_HA_1,ThroughputCapacity=2048,PreferredSubnetId=<desired primary subnet>,FsxAdminPassword=<new password>,DiskIopsConfiguration="{Mode=USER_PROVISIONED,Iops=40000"}
....
+
您可以在這裡找到適用於 FSX 「 cree-file-system 」的詳細命令列語法： https://docs.aws.amazon.com/cli/latest/reference/fsx/create-file-system.html[]
例如、您可以指定特定的 KMS 金鑰、而非指定 KMS 金鑰時所使用的預設 FSX 主要金鑰。

. 請等到 JSON 將檔案系統描述如下之後、「生命週期」狀態變更為「可用」：
+
....
[root@ip-172-31-33-69 ~]# aws fsx describe-file-systems  --region us-east-1 --file-system-ids fs-02ff04bab5ce01c7c
....
. fsxadmin 的密碼是第一次建立檔案系統時設定的密碼。
. 透過 fsxadmin 登入 FsxN 以驗證認證
+
....
[root@ip-172-31-33-69 ~]# ssh fsxadmin@198.19.250.244
The authenticity of host '198.19.250.244 (198.19.250.244)' can't be established.
ED25519 key fingerprint is SHA256:mgCyRXJfWRc2d/jOjFbMBsUcYOWjxoIky0ltHvVDL/Y.
This key is not known by any other names
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
Warning: Permanently added '198.19.250.244' (ED25519) to the list of known hosts.
(fsxadmin@198.19.250.244) Password:

This is your first recorded login.
....
. 在 FSxN 上建立儲存虛擬機器
+
....
[root@ip-172-31-33-69 ~]# aws fsx --region us-east-1 create-storage-virtual-machine --name svmkafkatest --file-system-id fs-02ff04bab5ce01c7c
....
. SSH 移至新建立的 NetApp ONTAP 檔案系統 FSX 、並使用以下範例命令在儲存虛擬機器中建立磁碟區、同樣地、我們也會建立 6 個磁碟區來進行此驗證。根據我們的驗證結果、保留預設成分（ 8 ）或以下成分、可為 Kafka 提供更好的效能。
+
....
FsxId02ff04bab5ce01c7c::*> volume create -volume kafkafsxN1 -state online -policy default -unix-permissions ---rwxr-xr-x -junction-active true -type RW -snapshot-policy none  -junction-path /kafkafsxN1 -aggr-list aggr1
....
. 將磁碟區的大小延伸至 2TB 、並安裝在連接路徑上。
+
....
FsxId02ff04bab5ce01c7c::*> volume size -volume kafkafsxN1 -new-size +2TB
vol size: Volume "svmkafkatest:kafkafsxN1" size set to 2.10t.

FsxId02ff04bab5ce01c7c::*> volume size -volume kafkafsxN2 -new-size +2TB
vol size: Volume "svmkafkatest:kafkafsxN2" size set to 2.10t.

FsxId02ff04bab5ce01c7c::*> volume size -volume kafkafsxN3 -new-size +2TB
vol size: Volume "svmkafkatest:kafkafsxN3" size set to 2.10t.

FsxId02ff04bab5ce01c7c::*> volume size -volume kafkafsxN4 -new-size +2TB
vol size: Volume "svmkafkatest:kafkafsxN4" size set to 2.10t.

FsxId02ff04bab5ce01c7c::*> volume size -volume kafkafsxN5 -new-size +2TB
vol size: Volume "svmkafkatest:kafkafsxN5" size set to 2.10t.

FsxId02ff04bab5ce01c7c::*> volume size -volume kafkafsxN6 -new-size +2TB
vol size: Volume "svmkafkatest:kafkafsxN6" size set to 2.10t.

FsxId02ff04bab5ce01c7c::*> volume show -vserver svmkafkatest -volume *
Vserver   Volume       Aggregate    State      Type       Size  Available Used%
--------- ------------ ------------ ---------- ---- ---------- ---------- -----
svmkafkatest
          kafkafsxN1   -            online     RW       2.10TB     1.99TB    0%
svmkafkatest
          kafkafsxN2   -            online     RW       2.10TB     1.99TB    0%
svmkafkatest
          kafkafsxN3   -            online     RW       2.10TB     1.99TB    0%
svmkafkatest
          kafkafsxN4   -            online     RW       2.10TB     1.99TB    0%
svmkafkatest
          kafkafsxN5   -            online     RW       2.10TB     1.99TB    0%
svmkafkatest
          kafkafsxN6   -            online     RW       2.10TB     1.99TB    0%
svmkafkatest
          svmkafkatest_root
                       aggr1        online     RW          1GB    968.1MB    0%
7 entries were displayed.

FsxId02ff04bab5ce01c7c::*> volume mount -volume kafkafsxN1 -junction-path /kafkafsxN1

FsxId02ff04bab5ce01c7c::*> volume mount -volume kafkafsxN2 -junction-path /kafkafsxN2

FsxId02ff04bab5ce01c7c::*> volume mount -volume kafkafsxN3 -junction-path /kafkafsxN3

FsxId02ff04bab5ce01c7c::*> volume mount -volume kafkafsxN4 -junction-path /kafkafsxN4

FsxId02ff04bab5ce01c7c::*> volume mount -volume kafkafsxN5 -junction-path /kafkafsxN5

FsxId02ff04bab5ce01c7c::*> volume mount -volume kafkafsxN6 -junction-path /kafkafsxN6
....
. 我們將 FSxN 處理量容量從每秒 2 GB 擴充至每秒 4 GB 、將 IOPS 擴充至 160000
+
....
[root@ip-172-31-33-69 ~]# aws fsx update-file-system --region us-east-1  --storage-capacity 5120 --ontap-configuration 'ThroughputCapacity=4096,DiskIopsConfiguration={Mode=USER_PROVISIONED,Iops=160000}' --file-system-id fs-02ff04bab5ce01c7c
....
+
您可以在這裡找到適用於 FSX 「 update-file-system 」的詳細命令列語法：
https://docs.aws.amazon.com/cli/latest/reference/fsx/update-file-system.html[]

. FSxN 磁碟區是以 nconnect 和預設選項安裝在 kafkar Brokers 中
+
image:aws-fsx-kafka-arch1.png["此影像顯示 FSxN 型 Kafka 叢集的架構。"]

+
** 運算：我們使用三節點的 Kafka 叢集、在專用伺服器上執行三節點的 zookeeper 群集。每個代理程式在 FSxN 執行個體上有六個 NFS 裝載點至六個磁碟區。
** 監控。我們使用兩個節點作為 Prometheus-Grafana 組合。為了產生工作負載、我們使用了一個獨立的三節點叢集、可以產生和使用這個Kafka叢集。
** 儲存設備。我們使用 FSxN 搭配六個 1TB 磁碟區。然後使用NFS掛載將磁碟區匯出至Kafka代理程式。






==== OpenMessage 基準測試組態。

我們使用的組態與 NetApp Cloud Volume ONTAP 相同、其詳細資料如下：
https://docs.netapp.com/us-en/netapp-solutions/data-analytics/kafka-nfs-performance-overview-and-validation-in-aws.html#architectural-setup[]



==== 測試方法

. Kafka 叢集是根據上述規格、使用 terraform 和 Ansible 來進行佈建。Terraform 用於使用適用於 Kafka 叢集的 AWS 執行個體來建置基礎架構、 Ansible 則在其上建置 Kafka 叢集。
. 使用上述工作負載組態和Sync驅動程式觸發OMB工作負載。
+
....
sudo bin/benchmark –drivers driver-kafka/kafka-sync.yaml workloads/1-topic-100-partitions-1kb.yaml
....
. 另一個工作負載是透過具有相同工作負載組態的處理量驅動程式觸發。
+
....
sudo bin/benchmark –drivers driver-kafka/kafka-throughput.yaml workloads/1-topic-100-partitions-1kb.yaml
....




==== 觀察

使用兩種不同類型的驅動程式來產生工作負載、以基準測試在NFS上執行的Kafka執行個體效能。驅動程式之間的差異在於記錄排清內容。

對於 Kafka Replication factor 1 和 FSxN ：

* 同步驅動程式一致產生的總處理量：約 3218 Mbps 、尖峰效能約 3652 Mbps 。
* 輸送量驅動程式一致產生的總處理量：約 3679 Mbps 、尖峰效能約 3908 Mbps 。


對於具有複寫係數 3 和 FSxN 的 Kafka ：

* 同步驅動程式一致產生的總處理量：約 1252 Mbps 、尖峰效能約 1382 Mbps 。
* 輸送量驅動程式一致產生的總處理量：約 1218 Mbps 、尖峰效能約 1328 Mbps 。


在 Kafka 複寫係數 3 中、 FSxN 的讀寫作業發生三次、在 Kafka 複寫係數 1 中、 FSxN 的讀寫作業是一次、因此在兩次驗證中、我們都能達到每秒 4GB 的最大處理量。

同步處理驅動程式可在記錄立即排入磁碟時產生一致的處理量、而處理量驅動程式則會在大量將記錄提交至磁碟時產生大量處理量。

這些處理量編號是針對指定的AWS組態所產生。為了達到更高的效能需求、可以進一步擴充和調整執行個體類型、以獲得更好的處理量。總處理量或總處理率是生產者和使用者速率的組合。

image:aws-fsxn-performance-rf-1-rf-3.png["此影像顯示 Kafka 搭配 RF1 和 RF3 的效能"]

下表顯示 Kafka 複寫係數 3 的每秒 2GB FSxn 和 4GB/ 秒效能。複寫因素 3 會在 FSxN 儲存設備上執行三次讀寫作業。處理量驅動程式的總速率為每秒 881 MB 、在 2GB/ 秒 FSxN 檔案系統上讀寫 Kafka 作業約為每秒 2.64 GB 、處理量驅動程式的總速率為每秒 1328 MB 、可讀寫 Kafka 作業約每秒 3.98 GB 。ther Kafka 效能是線性的、可根據 FSxN 處理量進行擴充。

image:aws-fsxn-2gb-4gb-scale.png["此影像顯示每秒 2 GB 和 4 GB/ 秒的橫向擴充效能。"]

下表顯示 EC2 執行個體與 FSxN 之間的效能（ Kafka Replication Factor ： 3 ）

image:aws-fsxn-ec2-fsxn-comparition.png["此影像顯示 RF3 中 EC2 與 FSxN 的效能比較。"]

link:kafka-nfs-performance-overview-and-validation-with-aff-on-premises.html["下一步：透過AFF 內部部署功能進行效能總覽與驗證。"]
