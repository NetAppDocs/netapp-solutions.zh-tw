---
sidebar: sidebar 
permalink: databases/aws_ora_ha_pacemaker.html 
keywords: Database, Oracle, AWS, EC2, FSx ONTAP, HA, Pacemaker 
summary: 本解決方案提供概觀與詳細資料、可在 Redhat Enterprise Linux （ RHEL ）上使用 Pacemaker 叢集功能、在 AWS EC2 中啟用 Oracle 高可用度（ HA ）、並在 Amazon FSX ONTAP 中透過 NFS 傳輸協定啟用資料庫儲存 HA 。 
---
= TR-4998 ： AWS EC2 中的 Oracle HA 搭配 Pacemaker 叢集和 FSX ONTAP
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ../media/


NetApp公司的Alleno Cao、Niyazz Mohamed

[role="lead"]
本解決方案提供概觀與詳細資料、可在 Redhat Enterprise Linux （ RHEL ）上使用 Pacemaker 叢集功能、在 AWS EC2 中啟用 Oracle 高可用度（ HA ）、並在 Amazon FSX ONTAP 中透過 NFS 傳輸協定啟用資料庫儲存 HA 。



== 目的

許多致力於在公有雲中自我管理及執行 Oracle 的客戶、都需要克服一些挑戰。其中一項挑戰是為 Oracle 資料庫提供高可用度。傳統上、 Oracle 客戶仰賴稱為「 Real Application Cluster 」或 RAC 的 Oracle 資料庫功能、在多個叢集節點上提供雙主動式交易支援。一個故障節點不會停止應用程式處理。可惜的是、 Oracle RAC 實作並不容易在許多常見的公有雲（例如 AWS EC2 ）中使用或支援。透過在 RHEL 和 Amazon FSX ONTAP 中運用內建的 Pacemaker 叢集（ PCS ）、客戶無需 Oracle RAC 授權成本、即可在運算和儲存設備上實現主動被動式叢集、以支援 AWS 雲端中的關鍵任務 Oracle 資料庫工作負載。

本文件說明如何在 RHEL 上設定 Pacemaker 叢集、在 EC2 和 Amazon FSX ONTAP 上部署 Oracle 資料庫、搭配 NFS 傳輸協定、在 Pacemaker for HA 中設定 Oracle 資源、以及在最常見的 HA 情境下進行驗證的示範。此解決方案也提供 NetApp SnapCenter UI 工具的快速 Oracle 資料庫備份、還原及複製資訊。

本解決方案可解決下列使用案例：

* 在 RHEL 中設定和組態 Pacemaker HA 叢集。
* 在 AWS EC2 和 Amazon FSX ONTAP 中部署 Oracle 資料庫 HA 。




== 目標對象

本解決方案適用於下列人員：

* DBA 希望在 AWS EC2 和 Amazon FSX ONTAP 中部署 Oracle 。
* 想要在 AWS EC2 和 Amazon FSX ONTAP 中測試 Oracle 工作負載的資料庫解決方案架構設計師。
* 想要在 AWS EC2 和 Amazon FSX ONTAP 中部署及管理 Oracle 資料庫的儲存管理員。
* 想要在 AWS EC2 和 Amazon FSX ONTAP 中備份 Oracle 資料庫的應用程式擁有者。




== 解決方案測試與驗證環境

此解決方案的測試與驗證是在實驗室環境中執行、可能與最終部署環境不符。請參閱一節 <<部署考量的關鍵因素>> 以取得更多資訊。



=== 架構

image:aws_ora_fsx_ec2_pcs_architecture.png["此影像提供 AWS EC2 中的 Oracle HA 與 Pacemaker 叢集和 FSX ONTAP 的詳細圖片。"]



=== 硬體與軟體元件

[cols="33%, 33%, 33%"]
|===


3+| *硬體* 


| Amazon FSX ONTAP 儲存設備 | AWS 提供的目前版本 | us-east-1 中的單一 AZ 、 1024 GiB 容量、 128 MB/s 處理量 


| DB 伺服器的 EC2 執行個體 | T2.xlarge / 4vcpU/16G | 兩個 EC2 T2 大型 EC2 執行個體、一個做為主要 DB 伺服器、另一個做為備用 DB 伺服器 


| Ansible 控制器的 VM | 4 個 vCPU 、 16GiB RAM | 一個 Linux VM 可在 NFS 上執行自動化 AWS EC2/FSX 資源配置和 Oracle 部署 


3+| *軟體* 


| RedHat Linux | RHEL Linux 8.6 （ LVM ） - x64 Gen2 | 已部署RedHat訂閱以進行測試 


| Oracle資料庫 | 版本 19.18 | 已套用 RU 修補程式 p34765931_190000_Linux-x86-64.zip 


| Oracle OPatch | 12.2.0.1.36 版 | 最新修補程式 p6880880_190000_Linux-x86-64.zip 


| 起搏器 | 0.10.18 版 | RedHat 適用於 RHEL 8.0 的高可用度附加元件 


| NFS | 3.0版 | Oracle DNFS 已啟用 


| Ansible | 核心 2 ． 2 | Python 3.6.8 
|===


=== AWS EC2/FSX 實驗室環境中的 Oracle 資料庫主動 / 被動組態

[cols="33%, 33%, 33%"]
|===


3+|  


| *伺服器* | *資料庫* | * 資料庫儲存 * 


| 主節點： orapm01/IP-172.30.15.111 | NTAP_PDB1 、 NTAP_PDB2 、 NTAP_PDB3 ） | /u01 、 /u02 、 /u03 NFS 會安裝在 Amazon FSX ONTAP 磁碟區上 


| 待命節點： orapm02/IP-172.30.15.5 | NTAP_PDB1 、 NTAP_PDB2 、 NTAP_PDB3 ）容錯移轉時 | /u01 、 /u02 、 /u03 NFS 會在容錯移轉時掛載 
|===


=== 部署考量的關鍵因素

* * Amazon FSX ONTAP HHA *根據預設、 Amazon FSX ONTAP 會在單一或多個可用區域的一對儲存控制器中進行資源配置。它以主動 / 被動的方式為關鍵任務資料庫工作負載提供儲存備援。儲存容錯移轉對終端使用者來說是透明的。儲存容錯移轉時不需要使用者介入。
* * PCS 資源群組和資源訂購。 *資源群組可讓多個具有相依性的資源在同一個叢集節點上執行。資源順序會以相反的方式強制執行資源啟動順序和關機順序。
* * 偏好的節點。 *Pacemaker 叢集專門部署於主動 / 被動式叢集（非 Pacemaker 的要求）、並與 FSX ONTAP 叢集同步。如果可用且位置限制、則使用中的 EC2 執行個體會設定為 Oracle 資源的慣用節點。
* * 待機節點上的 Fence 延遲。 *在雙節點 PC 叢集中、仲裁會人工設為 1 。當叢集節點之間發生通訊問題時、任一節點都可能會嘗試隔離另一個節點、這可能會導致資料毀損。在待機節點上設定延遲可減輕此問題、並可讓主節點在備用節點被圍起來時繼續提供服務。
* * 多個 AZ 部署考量。 *此解決方案是在單一可用性區域中部署及驗證。在多 AZ 部署中、需要額外的 AWS 網路資源、才能在可用性區域之間移動 PC 浮動 IP 。
* * Oracle 資料庫儲存配置。 *在本解決方案示範中、我們為測試資料庫 NTAP 配置三個資料庫磁碟區、以裝載 Oracle 二進位、資料和記錄檔。這些磁碟區會以 /u01 - 二進位、 /u02 - 資料和 /u03 - 記錄檔透過 NFS 裝載在 Oracle DB 伺服器上。雙控制檔是在 /u02 和 /u03 掛載點上設定、以提供備援。
* * DNFS 組態。 * 使用 DNFS （自 Oracle 11g 起提供）、在 DB VM 上執行的 Oracle 資料庫可提供比原生 NFS 用戶端更多的 I/O 。依預設、自動化 Oracle 部署會在 NFSv3 上設定 DNFS 。
* * 資料庫備份。 * NetApp 提供 SnapCenter 軟體套件、以方便使用者的 UI 介面進行資料庫備份、還原及複製。NetApp 建議實作這樣的管理工具、以快速（在一分鐘內）備份快照、快速（分鐘）資料庫還原及資料庫複製。




== 解決方案部署

以下各節提供在 AWS EC2 中部署和組態 Oracle 資料庫 HA 的逐步程序、其中包含 Pacemaker 叢集、以及用於資料庫儲存保護的 Amazon FSX ONTAP 。



=== 部署的先決條件

[%collapsible]
====
部署需要下列先決條件。

. 已設定AWS帳戶、並已在AWS帳戶中建立必要的VPC和網路區段。
. 在安裝最新版 Ansible 和 Git 的情況下、將 Linux VM 配置為 Ansible 控制器節點。如需詳細資訊、請參閱下列連結： link:../automation/getting-started.html["NetApp解決方案自動化入門"^] 在第 - 節中
`Setup the Ansible Control Node for CLI deployments on RHEL / CentOS` 或
`Setup the Ansible Control Node for CLI deployments on Ubuntu / Debian`。
+
在 Ansible 控制器和 EC2 執行個體 DB VM 之間啟用 ssh 公開 / 私密金鑰驗證。



====


=== 佈建 EC2 執行個體和 Amazon FSX ONTAP 儲存叢集

[%collapsible]
====
雖然 EC2 執行個體和 Amazon FSX ONTAP 可以從 AWS 主控台手動進行佈建、但建議您使用 NetApp Terraform 型自動化工具套件來自動化配置 EC2 執行個體和 FSX ONTAP 儲存叢集。以下是詳細的程序。

. 從 AWS CloudShell 或 Ansible 控制器 VM 複製一份 EC2 和 FSX ONTAP 的自動化工具套件複本。
+
[source, cli]
----
git clone https://bitbucket.ngage.netapp.com/scm/ns-bb/na_aws_fsx_ec2_deploy.git
----
+

NOTE: 如果工具組並非從 AWS CloudShell 執行、則您的 AWS 帳戶需要使用 AWS 使用者帳戶存取 / 秘密金鑰配對來執行 AWS CLI 驗證。

. 請參閱工具套件中的 readme.md 檔案。視需要修改 main.tf 和相關參數檔案、以取得必要的 AWS 資源。
+
....
An example of main.tf:

resource "aws_instance" "orapm01" {
  ami                           = var.ami
  instance_type                 = var.instance_type
  subnet_id                     = var.subnet_id
  key_name                      = var.ssh_key_name

  root_block_device {
    volume_type                 = "gp3"
    volume_size                 = var.root_volume_size
  }

  tags = {
    Name                        = var.ec2_tag1
  }
}

resource "aws_instance" "orapm02" {
  ami                           = var.ami
  instance_type                 = var.instance_type
  subnet_id                     = var.subnet_id
  key_name                      = var.ssh_key_name

  root_block_device {
    volume_type                 = "gp3"
    volume_size                 = var.root_volume_size
  }

  tags = {
    Name                        = var.ec2_tag2
  }
}

resource "aws_fsx_ontap_file_system" "fsx_01" {
  storage_capacity              = var.fs_capacity
  subnet_ids                    = var.subnet_ids
  preferred_subnet_id           = var.preferred_subnet_id
  throughput_capacity           = var.fs_throughput
  fsx_admin_password            = var.fsxadmin_password
  deployment_type               = var.deployment_type

  disk_iops_configuration {
    iops                        = var.iops
    mode                        = var.iops_mode
  }

  tags                          = {
    Name                        = var.fsx_tag
  }
}

resource "aws_fsx_ontap_storage_virtual_machine" "svm_01" {
  file_system_id                = aws_fsx_ontap_file_system.fsx_01.id
  name                          = var.svm_name
  svm_admin_password            = var.vsadmin_password
}

....
. 驗證並執行 Terraform 方案。成功執行會在目標 AWS 帳戶中建立兩個 EC2 執行個體和一個 FSX ONTAP 儲存叢集。自動化輸出會顯示 EC2 執行個體 IP 位址和 FSX ONTAP 叢集端點。
+
[source, cli]
----
terraform plan -out=main.plan
----
+
[source, cli]
----
terraform apply main.plan
----


這將完成適用於 Oracle 的 EC2 執行個體和 FSX ONTAP 資源配置。

====


=== Pacemaker 叢集設定

[%collapsible]
====
適用於 RHEL 的高可用度附加元件是叢集式系統、可為 Oracle 資料庫服務等關鍵正式作業服務提供可靠性、擴充性和可用度。在此使用案例示範中、會設定並設定雙節點節律器叢集、以支援主動 / 被動叢集案例中 Oracle 資料庫的高可用度。  

以 EC2 使用者身分登入 EC2 `both` 執行個體、完成 EC2 執行個體的下列工作：

. 移除 AWS Red Hat Update Infrastructure （ RHUI ）用戶端。
+
[source, cli]
----
sudo -i yum -y remove rh-amazon-rhui-client*
----
. 向 Red Hat 註冊 EC2 執行個體 VM 。
+
[source, cli]
----
sudo subscription-manager register --username xxxxxxxx --password 'xxxxxxxx' --auto-attach
----
. 啟用 RHEL 高可用度 rpm 。
+
[source, cli]
----
sudo subscription-manager config --rhsm.manage_repos=1
----
+
[source, cli]
----
sudo subscription-manager repos --enable=rhel-8-for-x86_64-highavailability-rpms
----
. 安裝起搏器和導板。
+
[source, cli]
----
sudo yum update -y
----
+
[source, cli]
----
sudo yum install pcs pacemaker fence-agents-aws
----
. 為所有叢集節點上的 hacluster 使用者建立密碼。對所有節點使用相同的密碼。
+
[source, cli]
----
sudo passwd hacluster
----
. 啟動 PCS 服務、並讓其在開機時啟動。
+
[source, cli]
----
sudo systemctl start pcsd.service
----
+
[source, cli]
----
sudo systemctl enable pcsd.service
----
. 驗證 PCSD 服務。
+
[source, cli]
----
sudo systemctl status pcsd
----
+
....
[ec2-user@ip-172-30-15-5 ~]$ sudo systemctl status pcsd
● pcsd.service - PCS GUI and remote configuration interface
   Loaded: loaded (/usr/lib/systemd/system/pcsd.service; enabled; vendor preset: disabled)
   Active: active (running) since Tue 2024-09-10 18:50:22 UTC; 33s ago
     Docs: man:pcsd(8)
           man:pcs(8)
 Main PID: 65302 (pcsd)
    Tasks: 1 (limit: 100849)
   Memory: 24.0M
   CGroup: /system.slice/pcsd.service
           └─65302 /usr/libexec/platform-python -Es /usr/sbin/pcsd

Sep 10 18:50:21 ip-172-30-15-5.ec2.internal systemd[1]: Starting PCS GUI and remote configuration interface...
Sep 10 18:50:22 ip-172-30-15-5.ec2.internal systemd[1]: Started PCS GUI and remote configuration interface.

....
. 將叢集節點新增至主機檔案。
+
[source, cli]
----
sudo vi /etc/hosts
----
+
....
[ec2-user@ip-172-30-15-5 ~]$ cat /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6

# cluster nodes
172.30.15.111   ip-172-30-15-111.ec2.internal
172.30.15.5     ip-172-30-15-5.ec2.internal

....
. 安裝並設定 awscli, 以連線至 AWS 帳戶。
+
[source, cli]
----
sudo yum install awscli
----
+
[source, cli]
----
sudo aws configure
----
+
....
[ec2-user@ip-172-30-15-111 ]# sudo aws configure
AWS Access Key ID [None]: XXXXXXXXXXXXXXXXX
AWS Secret Access Key [None]: XXXXXXXXXXXXXXXX
Default region name [None]: us-east-1
Default output format [None]: json

....
. 如果尚未安裝，請安裝 resource-agents 套件。
+
[source, cli]
----
sudo yum install resource-agents
----


在 `only one`叢集節點上、完成下列工作以建立 PC 叢集。

. 驗證 PC 使用者 hacluster 。
+
[source, cli]
----
sudo pcs host auth ip-172-30-15-5.ec2.internal ip-172-30-15-111.ec2.internal
----
+
....
[ec2-user@ip-172-30-15-111 ~]$ sudo pcs host auth ip-172-30-15-5.ec2.internal ip-172-30-15-111.ec2.internal
Username: hacluster
Password:
ip-172-30-15-111.ec2.internal: Authorized
ip-172-30-15-5.ec2.internal: Authorized

....
. 建立 PCS 叢集。
+
[source, cli]
----
sudo pcs cluster setup ora_ec2nfsx ip-172-30-15-5.ec2.internal ip-172-30-15-111.ec2.internal
----
+
....
[ec2-user@ip-172-30-15-111 ~]$ sudo pcs cluster setup ora_ec2nfsx ip-172-30-15-5.ec2.internal ip-172-30-15-111.ec2.internal
No addresses specified for host 'ip-172-30-15-5.ec2.internal', using 'ip-172-30-15-5.ec2.internal'
No addresses specified for host 'ip-172-30-15-111.ec2.internal', using 'ip-172-30-15-111.ec2.internal'
Destroying cluster on hosts: 'ip-172-30-15-111.ec2.internal', 'ip-172-30-15-5.ec2.internal'...
ip-172-30-15-5.ec2.internal: Successfully destroyed cluster
ip-172-30-15-111.ec2.internal: Successfully destroyed cluster
Requesting remove 'pcsd settings' from 'ip-172-30-15-111.ec2.internal', 'ip-172-30-15-5.ec2.internal'
ip-172-30-15-111.ec2.internal: successful removal of the file 'pcsd settings'
ip-172-30-15-5.ec2.internal: successful removal of the file 'pcsd settings'
Sending 'corosync authkey', 'pacemaker authkey' to 'ip-172-30-15-111.ec2.internal', 'ip-172-30-15-5.ec2.internal'
ip-172-30-15-111.ec2.internal: successful distribution of the file 'corosync authkey'
ip-172-30-15-111.ec2.internal: successful distribution of the file 'pacemaker authkey'
ip-172-30-15-5.ec2.internal: successful distribution of the file 'corosync authkey'
ip-172-30-15-5.ec2.internal: successful distribution of the file 'pacemaker authkey'
Sending 'corosync.conf' to 'ip-172-30-15-111.ec2.internal', 'ip-172-30-15-5.ec2.internal'
ip-172-30-15-111.ec2.internal: successful distribution of the file 'corosync.conf'
ip-172-30-15-5.ec2.internal: successful distribution of the file 'corosync.conf'
Cluster has been successfully set up.

....
. 啟用叢集。
+
[source, cli]
----
sudo pcs cluster enable --all
----
+
....
[ec2-user@ip-172-30-15-111 ~]$ sudo pcs cluster enable --all
ip-172-30-15-5.ec2.internal: Cluster Enabled
ip-172-30-15-111.ec2.internal: Cluster Enabled

....
. 啟動並驗證叢集。
+
[source, cli]
----
sudo pcs cluster start --all
----
+
[source, cli]
----
sudo pcs status
----
+
....
[ec2-user@ip-172-30-15-111 ~]$ sudo pcs status
Cluster name: ora_ec2nfsx

WARNINGS:
No stonith devices and stonith-enabled is not false

Cluster Summary:
  * Stack: corosync (Pacemaker is running)
  * Current DC: ip-172-30-15-111.ec2.internal (version 2.1.7-5.1.el8_10-0f7f88312) - partition with quorum
  * Last updated: Wed Sep 11 15:43:23 2024 on ip-172-30-15-111.ec2.internal
  * Last change:  Wed Sep 11 15:43:06 2024 by hacluster via hacluster on ip-172-30-15-111.ec2.internal
  * 2 nodes configured
  * 0 resource instances configured

Node List:
  * Online: [ ip-172-30-15-5.ec2.internal ip-172-30-15-111.ec2.internal ]


Full List of Resources:
  * No resources

Daemon Status:
  corosync: active/enabled
  pacemaker: active/enabled
  pcsd: active/enabled

....


這將完成 Pacemaker 叢集設定和初始設定。

====


=== Pacemaker 叢集隔離組態

[%collapsible]
====
正式作業叢集必須具備 Pacemaker 隔離組態。它可確保 AWS EC2 叢集上發生故障的節點會自動隔離、防止節點消耗叢集的資源、損害叢集的功能、或毀損共用資料。本節說明如何使用 Fence_AWS 隔離代理程式來設定叢集隔離。

. 以 root 使用者身分、輸入下列 AWS 中繼資料查詢、以取得每個 EC2 執行個體節點的執行個體 ID 。
+
[source, cli]
----
echo $(curl -s http://169.254.169.254/latest/meta-data/instance-id)
----
+
....
[root@ip-172-30-15-111 ec2-user]# echo $(curl -s http://169.254.169.254/latest/meta-data/instance-id)
i-0d8e7a0028371636f

or just get instance-id from AWS EC2 console
....
. 輸入下列命令以設定 Fence 裝置。使用 PCM1_host_map 命令、將 RHEL 主機名稱對應至執行個體 ID 。請使用 AWS 存取金鑰和 AWS 使用者帳戶的 AWS 秘密存取金鑰、您先前用於 AWS 驗證。
+
[source, cli]
----
sudo pcs stonith \
create clusterfence fence_aws access_key=XXXXXXXXXXXXXXXXX secret_key=XXXXXXXXXXXXXXXXXX \
region=us-east-1 pcmk_host_map="ip-172-30-15-111.ec2.internal:i-0d8e7a0028371636f;ip-172-30-15-5.ec2.internal:i-0bc54b315afb20a2e" \
power_timeout=240 pcmk_reboot_timeout=480 pcmk_reboot_retries=4
----
. 驗證隔離組態。
+
[source, cli]
----
pcs status
----
+
....
[root@ip-172-30-15-111 ec2-user]# pcs status
Cluster name: ora_ec2nfsx
Cluster Summary:
  * Stack: corosync (Pacemaker is running)
  * Current DC: ip-172-30-15-111.ec2.internal (version 2.1.7-5.1.el8_10-0f7f88312) - partition with quorum
  * Last updated: Wed Sep 11 21:17:18 2024 on ip-172-30-15-111.ec2.internal
  * Last change:  Wed Sep 11 21:16:40 2024 by root via root on ip-172-30-15-111.ec2.internal
  * 2 nodes configured
  * 1 resource instance configured

Node List:
  * Online: [ ip-172-30-15-5.ec2.internal ip-172-30-15-111.ec2.internal ]

Full List of Resources:
  * clusterfence        (stonith:fence_aws):     Started ip-172-30-15-111.ec2.internal

Daemon Status:
  corosync: active/enabled
  pacemaker: active/enabled
  pcsd: active/enabled

....
. 將 stonith 動作設為關、而非在叢集層級重新開機。
+
[source, cli]
----
pcs property set stonith-action=off
----
+
....

[root@ip-172-30-15-111 ec2-user]# pcs property config
Cluster Properties:
 cluster-infrastructure: corosync
 cluster-name: ora_ec2nfsx
 dc-version: 2.1.7-5.1.el8_10-0f7f88312
 have-watchdog: false
 last-lrm-refresh: 1726257586
 stonith-action: off

....
+

NOTE: 如果將 stonith 動作設為 off 、則圍籬叢集節點一開始就會關機。在 stonith POWER_timeout （ 240 秒）中定義的期間之後、會重新啟動圍籬節點、並重新加入叢集。

. 將待機節點的 Fence delay （隔離延遲）設為 10 秒。
+
[source, cli]
----
pcs stonith update clusterfence pcmk_delay_base="ip-172-30-15-111.ec2.internal:0;ip-172-30-15-5.ec2.internal:10s"
----
+
....
[root@ip-172-30-15-111 ec2-user]# pcs stonith config
Resource: clusterfence (class=stonith type=fence_aws)
  Attributes: clusterfence-instance_attributes
    access_key=XXXXXXXXXXXXXXXX
    pcmk_delay_base=ip-172-30-15-111.ec2.internal:0;ip-172-30-15-5.ec2.internal:10s
    pcmk_host_map=ip-172-30-15-111.ec2.internal:i-0d8e7a0028371636f;ip-172-30-15-5.ec2.internal:i-0bc54b315afb20a2e
    pcmk_reboot_retries=4
    pcmk_reboot_timeout=480
    power_timeout=240
    region=us-east-1
    secret_key=XXXXXXXXXXXXXXXX
  Operations:
    monitor: clusterfence-monitor-interval-60s
      interval=60s

....



NOTE: 執行 `pcs stonith refresh`命令以重新整理停止的 stonith Fence 代理程式或清除失敗的 stonith 資源動作。

====


=== 在 PC 叢集中部署 Oracle 資料庫

[%collapsible]
====
我們建議使用 NetApp 提供的 Ansible 教戰手冊、在 PCS 叢集上以預先定義的參數執行資料庫安裝和組態工作。對於這項自動化 Oracle 部署、三個使用者定義的參數檔案需要使用者輸入才能執行教戰手冊。

* 主機：定義自動化教戰手冊所針對的目標。
* VARS/vars.yml - 定義適用於所有目標的變數的全域變數檔案。
* host_vars/host_name.yml - 定義僅適用於命名目標的變數的本機變數檔案。在我們的使用案例中、這些是 Oracle DB 伺服器。


除了這些使用者定義的變數檔案之外、還有幾個預設變數檔案、其中包含預設參數、除非必要、否則不需要變更。以下顯示在 PCS 叢集組態中於 AWS EC2 和 FSX ONTAP 中自動部署 Oracle 的詳細資料。

. 從 Ansible 控制器管理使用者主目錄中、複製 NetApp Oracle 部署自動化工具套件 for NFS 的複本。
+
[source, cli]
----
git clone https://bitbucket.ngage.netapp.com/scm/ns-bb/na_oracle_deploy_nfs.git
----
+

NOTE: Ansible 控制器可與資料庫 EC2 執行個體位於同一部 VPC 、或只要兩者之間有網路連線、則可位於內部部署。

. 在 hosts 參數檔中填入使用者定義的參數。以下是典型主機檔案組態的範例。
+
....

[admin@ansiblectl na_oracle_deploy_nfs]$ cat hosts
#Oracle hosts
[oracle]
orapm01 ansible_host=172.30.15.111 ansible_ssh_private_key_file=ec2-user.pem
orapm02 ansible_host=172.30.15.5 ansible_ssh_private_key_file=ec2-user.pem

....
. 在 vars/vars.yml 參數檔案中填入使用者定義的參數。以下是典型 vars.yml 檔案組態的範例。
+
....

[admin@ansiblectl na_oracle_deploy_nfs]$ cat vars/vars.yml
######################################################################
###### Oracle 19c deployment user configuration variables       ######
###### Consolidate all variables from ONTAP, linux and oracle   ######
######################################################################

###########################################
### ONTAP env specific config variables ###
###########################################

# Prerequisite to create three volumes in NetApp ONTAP storage from System Manager or cloud dashboard with following naming convention:
# db_hostname_u01 - Oracle binary
# db_hostname_u02 - Oracle data
# db_hostname_u03 - Oracle redo
# It is important to strictly follow the name convention or the automation will fail.


###########################################
### Linux env specific config variables ###
###########################################

redhat_sub_username: xxxxxxxx
redhat_sub_password: "xxxxxxxx"


####################################################
### DB env specific install and config variables ###
####################################################

# Database domain name
db_domain: ec2.internal

# Set initial password for all required Oracle passwords. Change them after installation.
initial_pwd_all: "xxxxxxxx"

....
. 在 host_vars/host_name.yml 參數檔案中填入使用者定義的參數。以下是典型的 host_vars/host_name.yml 檔案組態範例。
+
....

[admin@ansiblectl na_oracle_deploy_nfs]$ cat host_vars/orapm01.yml
# User configurable Oracle host specific parameters

# Database SID. By default, a container DB is created with 3 PDBs within the CDB
oracle_sid: NTAP

# CDB is created with SGA at 75% of memory_limit, MB. Consider how many databases to be hosted on the node and
# how much ram to be allocated to each DB. The grand total of SGA should not exceed 75% available RAM on node.
memory_limit: 8192

# Local NFS lif ip address to access database volumes
nfs_lif: 172.30.15.95

....
+

NOTE: NFS_LIF 位址可從上一節的自動 EC2 和 FSX ONTAP 部署所輸出的 FSX ONTAP 叢集端點擷取。

. 從 AWS FSX 主控台建立資料庫磁碟區。請務必使用 PCS 主要節點主機名稱（ orapm01 ）作為磁碟區的首碼、如下所示。
+
image:aws_ora_fsx_ec2_pcs_01.png["此映像可從 AWS FSX 主控台提供 Amazon FSX ONTAP Volume 資源配置"] image:aws_ora_fsx_ec2_pcs_02.png["此映像可從 AWS FSX 主控台提供 Amazon FSX ONTAP Volume 資源配置"] image:aws_ora_fsx_ec2_pcs_03.png["此映像可從 AWS FSX 主控台提供 Amazon FSX ONTAP Volume 資源配置"] image:aws_ora_fsx_ec2_pcs_04.png["此映像可從 AWS FSX 主控台提供 Amazon FSX ONTAP Volume 資源配置"] image:aws_ora_fsx_ec2_pcs_05.png["此映像可從 AWS FSX 主控台提供 Amazon FSX ONTAP Volume 資源配置"]

. 在 PC 主節點 EC2 執行個體 ip-172-30-15-111.ec2.internal /tmp/archive 目錄上執行 Oracle 19c 安裝檔案、並具有 777 權限。
+
....
installer_archives:
  - "LINUX.X64_193000_db_home.zip"
  - "p34765931_190000_Linux-x86-64.zip"
  - "p6880880_190000_Linux-x86-64.zip"
....
. 執行 Linux 組態的教戰手冊 `all nodes`。
+
[source, cli]
----
ansible-playbook -i hosts 2-linux_config.yml -u ec2-user -e @vars/vars.yml
----
+
....
[admin@ansiblectl na_oracle_deploy_nfs]$ ansible-playbook -i hosts 2-linux_config.yml -u ec2-user -e @vars/vars.yml

PLAY [Linux Setup and Storage Config for Oracle] ****************************************************************************************************************************************************************************************************************************************************************************

TASK [Gathering Facts] ******************************************************************************************************************************************************************************************************************************************************************************************************
ok: [orapm01]
ok: [orapm02]

TASK [linux : Configure RedHat 7 for Oracle DB installation] ****************************************************************************************************************************************************************************************************************************************************************
skipping: [orapm01]
skipping: [orapm02]

TASK [linux : Configure RedHat 8 for Oracle DB installation] ****************************************************************************************************************************************************************************************************************************************************************
included: /home/admin/na_oracle_deploy_nfs/roles/linux/tasks/rhel8_config.yml for orapm01, orapm02

TASK [linux : Register subscriptions for RedHat Server] *********************************************************************************************************************************************************************************************************************************************************************
ok: [orapm01]
ok: [orapm02]
.
.
.
....
. 執行 Oracle 組態的教戰手冊 `only on primary node`（在 hosts 檔案中註解待命節點）。
+
[source, cli]
----
ansible-playbook -i hosts 4-oracle_config.yml -u ec2-user -e @vars/vars.yml --skip-tags "enable_db_start_shut"
----
+
....
[admin@ansiblectl na_oracle_deploy_nfs]$ ansible-playbook -i hosts 4-oracle_config.yml -u ec2-user -e @vars/vars.yml --skip-tags "enable_db_start_shut"

PLAY [Oracle installation and configuration] ********************************************************************************************************************************************************************************************************************************************************************************

TASK [Gathering Facts] ******************************************************************************************************************************************************************************************************************************************************************************************************
ok: [orapm01]

TASK [oracle : Oracle software only install] ********************************************************************************************************************************************************************************************************************************************************************************
included: /home/admin/na_oracle_deploy_nfs/roles/oracle/tasks/oracle_install.yml for orapm01

TASK [oracle : Create mount points for NFS file systems / Mount NFS file systems on Oracle hosts] ***************************************************************************************************************************************************************************************************************************
included: /home/admin/na_oracle_deploy_nfs/roles/oracle/tasks/oracle_mount_points.yml for orapm01

TASK [oracle : Create mount points for NFS file systems] ********************************************************************************************************************************************************************************************************************************************************************
changed: [orapm01] => (item=/u01)
changed: [orapm01] => (item=/u02)
changed: [orapm01] => (item=/u03)
.
.
.
....
. 部署資料庫之後、請在主要節點上的 /etc/fstab 中註解 /u01 、 /u02 、 /u03 裝載、因為裝載點僅由 PC 管理。
+
[source, cli]
----
sudo vi /etc/fstab
----
+
....

[root@ip-172-30-15-111 ec2-user]# cat /etc/fstab
UUID=eaa1f38e-de0f-4ed5-a5b5-2fa9db43bb38       /       xfs     defaults        0       0
/mnt/swapfile swap swap defaults 0 0
#172.30.15.95:/orapm01_u01 /u01 nfs rw,bg,hard,vers=3,proto=tcp,timeo=600,rsize=65536,wsize=65536 0 0
#172.30.15.95:/orapm01_u02 /u02 nfs rw,bg,hard,vers=3,proto=tcp,timeo=600,rsize=65536,wsize=65536 0 0
#172.30.15.95:/orapm01_u03 /u03 nfs rw,bg,hard,vers=3,proto=tcp,timeo=600,rsize=65536,wsize=65536 0 0

....
. 將 /etc/oratab /etc/oraInst.loc 、 /home/oracle/.bash_profile 複製到備用節點。確保維持適當的檔案擁有權和權限。
. 關閉主要節點上的資料庫、接聽程式及 umount /u01 、 /u02 、 /u03 。
+
....

[root@ip-172-30-15-111 ec2-user]# su - oracle
Last login: Wed Sep 18 16:51:02 UTC 2024
[oracle@ip-172-30-15-111 ~]$ sqlplus / as sysdba

SQL*Plus: Release 19.0.0.0.0 - Production on Wed Sep 18 16:51:16 2024
Version 19.18.0.0.0

Copyright (c) 1982, 2022, Oracle.  All rights reserved.


Connected to:
Oracle Database 19c Enterprise Edition Release 19.0.0.0.0 - Production
Version 19.18.0.0.0

SQL> shutdown immediate;

SQL> exit
Disconnected from Oracle Database 19c Enterprise Edition Release 19.0.0.0.0 - Production
Version 19.18.0.0.0
[oracle@ip-172-30-15-111 ~]$ lsnrctl stop listener.ntap

[oracle@ip-172-30-15-111 ~]$ exit
logout
[root@ip-172-30-15-111 ec2-user]# umount /u01
[root@ip-172-30-15-111 ec2-user]# umount /u02
[root@ip-172-30-15-111 ec2-user]# umount /u03

....
. 在備用節點 IP-172-30-15-5 上建立掛載點。
+
[source, cli]
----
mkdir /u01
mkdir /u02
mkdir /u03
----
. 在待命節點 IP-172-30-15-5 上掛載 FSX ONTAP 資料庫磁碟區。
+
[source, cli]
----
mount -t nfs 172.30.15.95:/orapm01_u01 /u01 -o rw,bg,hard,vers=3,proto=tcp,timeo=600,rsize=65536,wsize=65536
----
+
[source, cli]
----
mount -t nfs 172.30.15.95:/orapm01_u02 /u02 -o rw,bg,hard,vers=3,proto=tcp,timeo=600,rsize=65536,wsize=65536
----
+
[source, cli]
----
mount -t nfs 172.30.15.95:/orapm01_u03 /u03 -o rw,bg,hard,vers=3,proto=tcp,timeo=600,rsize=65536,wsize=65536
----
+
....

[root@ip-172-30-15-5 ec2-user]# df -h
Filesystem                 Size  Used Avail Use% Mounted on
devtmpfs                   7.7G     0  7.7G   0% /dev
tmpfs                      7.7G   33M  7.7G   1% /dev/shm
tmpfs                      7.7G   17M  7.7G   1% /run
tmpfs                      7.7G     0  7.7G   0% /sys/fs/cgroup
/dev/xvda2                  50G   21G   30G  41% /
tmpfs                      1.6G     0  1.6G   0% /run/user/1000
172.30.15.95:/orapm01_u01   48T   47T  844G  99% /u01
172.30.15.95:/orapm01_u02  285T  285T  844G 100% /u02
172.30.15.95:/orapm01_u03  190T  190T  844G 100% /u03

....
. 變更為 Oracle 使用者、重新連結二進位檔案。
+
....

[root@ip-172-30-15-5 ec2-user]# su - oracle
Last login: Thu Sep 12 18:09:03 UTC 2024 on pts/0
[oracle@ip-172-30-15-5 ~]$ env | grep ORA
ORACLE_SID=NTAP
ORACLE_HOME=/u01/app/oracle/product/19.0.0/NTAP
[oracle@ip-172-30-15-5 ~]$ cd $ORACLE_HOME/bin
[oracle@ip-172-30-15-5 bin]$ ./relink
writing relink log to: /u01/app/oracle/product/19.0.0/NTAP/install/relinkActions2024-09-12_06-21-40PM.log

....
. 將 nfs 程式庫複製回 ODM 資料夾。重新連結可能會遺失 dfns 程式庫檔案。
+
....

[oracle@ip-172-30-15-5 odm]$ cd /u01/app/oracle/product/19.0.0/NTAP/rdbms/lib/odm
[oracle@ip-172-30-15-5 odm]$ cp ../../../lib/libnfsodm19.so .

....
. 啟動資料庫以在待命節點 IP-172-30-15-5 上驗證。
+
....

[oracle@ip-172-30-15-5 odm]$ sqlplus / as sysdba

SQL*Plus: Release 19.0.0.0.0 - Production on Thu Sep 12 18:30:04 2024
Version 19.18.0.0.0

Copyright (c) 1982, 2022, Oracle.  All rights reserved.

Connected to an idle instance.

SQL> startup;
ORACLE instance started.

Total System Global Area 6442449688 bytes
Fixed Size                  9177880 bytes
Variable Size            1090519040 bytes
Database Buffers         5335154688 bytes
Redo Buffers                7598080 bytes
Database mounted.
Database opened.
SQL> select name, open_mode from v$database;

NAME      OPEN_MODE
--------- --------------------
NTAP      READ WRITE

SQL> show pdbs

    CON_ID CON_NAME                       OPEN MODE  RESTRICTED
---------- ------------------------------ ---------- ----------
         2 PDB$SEED                       READ ONLY  NO
         3 NTAP_PDB1                      READ WRITE NO
         4 NTAP_PDB2                      READ WRITE NO
         5 NTAP_PDB3                      READ WRITE NO


....
. 將資料庫和容錯回復資料庫關機至主要節點 IP-172-30-15-111 。
+
....

SQL> shutdown immediate;
Database closed.
Database dismounted.
ORACLE instance shut down.
SQL> exit

[root@ip-172-30-15-5 ec2-user]# df -h
Filesystem                 Size  Used Avail Use% Mounted on
devtmpfs                   7.7G     0  7.7G   0% /dev
tmpfs                      7.7G   33M  7.7G   1% /dev/shm
tmpfs                      7.7G   17M  7.7G   1% /run
tmpfs                      7.7G     0  7.7G   0% /sys/fs/cgroup
/dev/xvda2                  50G   21G   30G  41% /
tmpfs                      1.6G     0  1.6G   0% /run/user/1000
172.30.15.95:/orapm01_u01   48T   47T  844G  99% /u01
172.30.15.95:/orapm01_u02  285T  285T  844G 100% /u02
172.30.15.95:/orapm01_u03  190T  190T  844G 100% /u03

[root@ip-172-30-15-5 ec2-user]# umount /u01
[root@ip-172-30-15-5 ec2-user]# umount /u02
[root@ip-172-30-15-5 ec2-user]# umount /u03

[root@ip-172-30-15-111 ec2-user]# mount -t nfs 172.30.15.95:/orapm01_u01 /u01 -o rw,bg,hard,vers=3,proto=tcp,timeo=600,rsize=65536,wsize=65536
mount: (hint) your fstab has been modified, but systemd still uses
       the old version; use 'systemctl daemon-reload' to reload.
[root@ip-172-30-15-111 ec2-user]# mount -t nfs 172.30.15.95:/orapm01_u02 /u02 -o rw,bg,hard,vers=3,proto=tcp,timeo=600,rsize=65536,wsize=65536
mount: (hint) your fstab has been modified, but systemd still uses
       the old version; use 'systemctl daemon-reload' to reload.
[root@ip-172-30-15-111 ec2-user]# mount -t nfs 172.30.15.95:/orapm01_u03 /u03 -o rw,bg,hard,vers=3,proto=tcp,timeo=600,rsize=65536,wsize=65536
mount: (hint) your fstab has been modified, but systemd still uses
       the old version; use 'systemctl daemon-reload' to reload.
[root@ip-172-30-15-111 ec2-user]# df -h
Filesystem                 Size  Used Avail Use% Mounted on
devtmpfs                   7.7G     0  7.7G   0% /dev
tmpfs                      7.8G   48M  7.7G   1% /dev/shm
tmpfs                      7.8G   33M  7.7G   1% /run
tmpfs                      7.8G     0  7.8G   0% /sys/fs/cgroup
/dev/xvda2                  50G   29G   22G  58% /
tmpfs                      1.6G     0  1.6G   0% /run/user/1000
172.30.15.95:/orapm01_u01   48T   47T  844G  99% /u01
172.30.15.95:/orapm01_u02  285T  285T  844G 100% /u02
172.30.15.95:/orapm01_u03  190T  190T  844G 100% /u03
[root@ip-172-30-15-111 ec2-user]# su - oracle
Last login: Thu Sep 12 18:13:34 UTC 2024 on pts/1
[oracle@ip-172-30-15-111 ~]$ sqlplus / as sysdba

SQL*Plus: Release 19.0.0.0.0 - Production on Thu Sep 12 18:38:46 2024
Version 19.18.0.0.0

Copyright (c) 1982, 2022, Oracle.  All rights reserved.

Connected to an idle instance.

SQL> startup;
ORACLE instance started.

Total System Global Area 6442449688 bytes
Fixed Size                  9177880 bytes
Variable Size            1090519040 bytes
Database Buffers         5335154688 bytes
Redo Buffers                7598080 bytes
Database mounted.
Database opened.
SQL> exit
Disconnected from Oracle Database 19c Enterprise Edition Release 19.0.0.0.0 - Production
Version 19.18.0.0.0
[oracle@ip-172-30-15-111 ~]$ lsnrctl start listener.ntap

LSNRCTL for Linux: Version 19.0.0.0.0 - Production on 12-SEP-2024 18:39:17

Copyright (c) 1991, 2022, Oracle.  All rights reserved.

Starting /u01/app/oracle/product/19.0.0/NTAP/bin/tnslsnr: please wait...

TNSLSNR for Linux: Version 19.0.0.0.0 - Production
System parameter file is /u01/app/oracle/product/19.0.0/NTAP/network/admin/listener.ora
Log messages written to /u01/app/oracle/diag/tnslsnr/ip-172-30-15-111/listener.ntap/alert/log.xml
Listening on: (DESCRIPTION=(ADDRESS=(PROTOCOL=tcp)(HOST=ip-172-30-15-111.ec2.internal)(PORT=1521)))
Listening on: (DESCRIPTION=(ADDRESS=(PROTOCOL=ipc)(KEY=EXTPROC1521)))

Connecting to (DESCRIPTION=(ADDRESS=(PROTOCOL=TCP)(HOST=ip-172-30-15-111.ec2.internal)(PORT=1521)))
STATUS of the LISTENER
------------------------
Alias                     listener.ntap
Version                   TNSLSNR for Linux: Version 19.0.0.0.0 - Production
Start Date                12-SEP-2024 18:39:17
Uptime                    0 days 0 hr. 0 min. 0 sec
Trace Level               off
Security                  ON: Local OS Authentication
SNMP                      OFF
Listener Parameter File   /u01/app/oracle/product/19.0.0/NTAP/network/admin/listener.ora
Listener Log File         /u01/app/oracle/diag/tnslsnr/ip-172-30-15-111/listener.ntap/alert/log.xml
Listening Endpoints Summary...
  (DESCRIPTION=(ADDRESS=(PROTOCOL=tcp)(HOST=ip-172-30-15-111.ec2.internal)(PORT=1521)))
  (DESCRIPTION=(ADDRESS=(PROTOCOL=ipc)(KEY=EXTPROC1521)))
The listener supports no services
The command completed successfully

....


====


=== 設定用於 PC 管理的 Oracle 資源

[%collapsible]
====
設定 Pacemaker 叢集的目標是在 AWS EC2 和 FSX ONTAP 環境中、設定主動 / 被動高可用度解決方案、在發生故障時、使用者介入的次數最少。以下說明 PC 管理的 Oracle 資源組態。

. 身為主要 EC2 執行個體 IP-172-30-15-111 上的 root 使用者、請在 VPC CIDR 區塊中建立次要私有 IP 位址、並將未使用的私有 IP 位址作為浮動 IP 。在此過程中、建立次要私有 IP 位址所屬的 Oracle 資源群組。
+
[source, cli]
----
pcs resource create privip ocf:heartbeat:awsvip secondary_private_ip=172.30.15.33 --group oracle
----
+
....

[root@ip-172-30-15-111 ec2-user]# pcs status
Cluster name: ora_ec2nfsx
Cluster Summary:
  * Stack: corosync (Pacemaker is running)
  * Current DC: ip-172-30-15-111.ec2.internal (version 2.1.7-5.1.el8_10-0f7f88312) - partition with quorum
  * Last updated: Fri Sep 13 16:25:35 2024 on ip-172-30-15-111.ec2.internal
  * Last change:  Fri Sep 13 16:25:23 2024 by root via root on ip-172-30-15-111.ec2.internal
  * 2 nodes configured
  * 2 resource instances configured

Node List:
  * Online: [ ip-172-30-15-5.ec2.internal ip-172-30-15-111.ec2.internal ]

Full List of Resources:
  * clusterfence        (stonith:fence_aws):     Started ip-172-30-15-111.ec2.internal
  * Resource Group: oracle:
    * privip    (ocf::heartbeat:awsvip):         Started ip-172-30-15-5.ec2.internal

Daemon Status:
  corosync: active/enabled
  pacemaker: active/enabled
  pcsd: active/enabled

....
+

NOTE: 如果發生在備用叢集節點上建立 privip 的情況、請將其移至主要節點、如下所示。

. 在叢集節點之間移動資源。
+
[source, cli]
----
pcs resource move privip ip-172-30-15-111.ec2.internal
----
+
....

[root@ip-172-30-15-111 ec2-user]# pcs resource move privip ip-172-30-15-111.ec2.internal
Warning: A move constraint has been created and the resource 'privip' may or may not move depending on other configuration
[root@ip-172-30-15-111 ec2-user]# pcs status
Cluster name: ora_ec2nfsx

WARNINGS:
Following resources have been moved and their move constraints are still in place: 'privip'
Run 'pcs constraint location' or 'pcs resource clear <resource id>' to view or remove the constraints, respectively

Cluster Summary:
  * Stack: corosync (Pacemaker is running)
  * Current DC: ip-172-30-15-111.ec2.internal (version 2.1.7-5.1.el8_10-0f7f88312) - partition with quorum
  * Last updated: Fri Sep 13 16:26:38 2024 on ip-172-30-15-111.ec2.internal
  * Last change:  Fri Sep 13 16:26:27 2024 by root via root on ip-172-30-15-111.ec2.internal
  * 2 nodes configured
  * 2 resource instances configured

Node List:
  * Online: [ ip-172-30-15-5.ec2.internal ip-172-30-15-111.ec2.internal ]

Full List of Resources:
  * clusterfence        (stonith:fence_aws):     Started ip-172-30-15-111.ec2.internal
  * Resource Group: oracle:
    * privip    (ocf::heartbeat:awsvip):         Started ip-172-30-15-111.ec2.internal (Monitoring)

Daemon Status:
  corosync: active/enabled
  pacemaker: active/enabled
  pcsd: active/enabled

....
. 為 Oracle 建立虛擬 IP （ VIP ）。虛擬 IP 會視需要在主要節點和待命節點之間浮動。
+
[source, cli]
----
pcs resource create vip ocf:heartbeat:IPaddr2 ip=172.30.15.33 cidr_netmask=25 nic=eth0 op monitor interval=10s --group oracle
----
+
....

[root@ip-172-30-15-111 ec2-user]# pcs resource create vip ocf:heartbeat:IPaddr2 ip=172.30.15.33 cidr_netmask=25 nic=eth0 op monitor interval=10s --group oracle
[root@ip-172-30-15-111 ec2-user]# pcs status
Cluster name: ora_ec2nfsx

WARNINGS:
Following resources have been moved and their move constraints are still in place: 'privip'
Run 'pcs constraint location' or 'pcs resource clear <resource id>' to view or remove the constraints, respectively

Cluster Summary:
  * Stack: corosync (Pacemaker is running)
  * Current DC: ip-172-30-15-111.ec2.internal (version 2.1.7-5.1.el8_10-0f7f88312) - partition with quorum
  * Last updated: Fri Sep 13 16:27:34 2024 on ip-172-30-15-111.ec2.internal
  * Last change:  Fri Sep 13 16:27:24 2024 by root via root on ip-172-30-15-111.ec2.internal
  * 2 nodes configured
  * 3 resource instances configured

Node List:
  * Online: [ ip-172-30-15-5.ec2.internal ip-172-30-15-111.ec2.internal ]

Full List of Resources:
  * clusterfence        (stonith:fence_aws):     Started ip-172-30-15-111.ec2.internal
  * Resource Group: oracle:
    * privip    (ocf::heartbeat:awsvip):         Started ip-172-30-15-111.ec2.internal
    * vip       (ocf::heartbeat:IPaddr2):        Started ip-172-30-15-111.ec2.internal

Daemon Status:
  corosync: active/enabled
  pacemaker: active/enabled
  pcsd: active/enabled

....
. 以 Oracle 使用者身分更新 listener.ora 和 tnsnames.ora 檔案、以指向 VIP 位址。重新啟動接聽程式。如果 DB 需要向偵聽器註冊、則會退回資料庫。
+
[source, cli]
----
vi $ORACLE_HOME/network/admin/listener.ora
----
+
[source, cli]
----
vi $ORACLE_HOME/network/admin/tnsnames.ora
----
+
....

[oracle@ip-172-30-15-111 admin]$ cat listener.ora
# listener.ora Network Configuration File: /u01/app/oracle/product/19.0.0/NTAP/network/admin/listener.ora
# Generated by Oracle configuration tools.

LISTENER.NTAP =
  (DESCRIPTION_LIST =
    (DESCRIPTION =
      (ADDRESS = (PROTOCOL = TCP)(HOST = 172.30.15.33)(PORT = 1521))
      (ADDRESS = (PROTOCOL = IPC)(KEY = EXTPROC1521))
    )
  )

[oracle@ip-172-30-15-111 admin]$ cat tnsnames.ora
# tnsnames.ora Network Configuration File: /u01/app/oracle/product/19.0.0/NTAP/network/admin/tnsnames.ora
# Generated by Oracle configuration tools.

NTAP =
  (DESCRIPTION =
    (ADDRESS = (PROTOCOL = TCP)(HOST = 172.30.15.33)(PORT = 1521))
    (CONNECT_DATA =
      (SERVER = DEDICATED)
      (SERVICE_NAME = NTAP.ec2.internal)
    )
  )

LISTENER_NTAP =
  (ADDRESS = (PROTOCOL = TCP)(HOST = 172.30.15.33)(PORT = 1521))


[oracle@ip-172-30-15-111 admin]$ lsnrctl status listener.ntap

LSNRCTL for Linux: Version 19.0.0.0.0 - Production on 13-SEP-2024 18:28:17

Copyright (c) 1991, 2022, Oracle.  All rights reserved.

Connecting to (DESCRIPTION=(ADDRESS=(PROTOCOL=TCP)(HOST=172.30.15.33)(PORT=1521)))
STATUS of the LISTENER
------------------------
Alias                     listener.ntap
Version                   TNSLSNR for Linux: Version 19.0.0.0.0 - Production
Start Date                13-SEP-2024 18:15:51
Uptime                    0 days 0 hr. 12 min. 25 sec
Trace Level               off
Security                  ON: Local OS Authentication
SNMP                      OFF
Listener Parameter File   /u01/app/oracle/product/19.0.0/NTAP/network/admin/listener.ora
Listener Log File         /u01/app/oracle/diag/tnslsnr/ip-172-30-15-111/listener.ntap/alert/log.xml
Listening Endpoints Summary...
  (DESCRIPTION=(ADDRESS=(PROTOCOL=tcp)(HOST=172.30.15.33)(PORT=1521)))
  (DESCRIPTION=(ADDRESS=(PROTOCOL=ipc)(KEY=EXTPROC1521)))
  (DESCRIPTION=(ADDRESS=(PROTOCOL=tcps)(HOST=ip-172-30-15-111.ec2.internal)(PORT=5500))(Security=(my_wallet_directory=/u01/app/oracle/product/19.0.0/NTAP/admin/NTAP/xdb_wallet))(Presentation=HTTP)(Session=RAW))
Services Summary...
Service "21f0b5cc1fa290e2e0636f0f1eacfd43.ec2.internal" has 1 instance(s).
  Instance "NTAP", status READY, has 1 handler(s) for this service...
Service "21f0b74445329119e0636f0f1eacec03.ec2.internal" has 1 instance(s).
  Instance "NTAP", status READY, has 1 handler(s) for this service...
Service "21f0b83929709164e0636f0f1eacacc3.ec2.internal" has 1 instance(s).
  Instance "NTAP", status READY, has 1 handler(s) for this service...
Service "NTAP.ec2.internal" has 1 instance(s).
  Instance "NTAP", status READY, has 1 handler(s) for this service...
Service "NTAPXDB.ec2.internal" has 1 instance(s).
  Instance "NTAP", status READY, has 1 handler(s) for this service...
Service "ntap_pdb1.ec2.internal" has 1 instance(s).
  Instance "NTAP", status READY, has 1 handler(s) for this service...
Service "ntap_pdb2.ec2.internal" has 1 instance(s).
  Instance "NTAP", status READY, has 1 handler(s) for this service...
Service "ntap_pdb3.ec2.internal" has 1 instance(s).
  Instance "NTAP", status READY, has 1 handler(s) for this service...
The command completed successfully

**Oracle listener now listens on vip for database connection**
....
. 將 /u01 、 /u02 、 /u03 裝載點新增至 Oracle 資源群組。
+
[source, cli]
----
pcs resource create u01 ocf:heartbeat:Filesystem device='172.30.15.95:/orapm01_u01' directory='/u01' fstype='nfs' options='rw,bg,hard,vers=3,proto=tcp,timeo=600,rsize=65536,wsize=65536' --group oracle
----
+
[source, cli]
----
pcs resource create u02 ocf:heartbeat:Filesystem device='172.30.15.95:/orapm01_u02' directory='/u02' fstype='nfs' options='rw,bg,hard,vers=3,proto=tcp,timeo=600,rsize=65536,wsize=65536' --group oracle
----
+
[source, cli]
----
pcs resource create u03 ocf:heartbeat:Filesystem device='172.30.15.95:/orapm01_u03' directory='/u03' fstype='nfs' options='rw,bg,hard,vers=3,proto=tcp,timeo=600,rsize=65536,wsize=65536' --group oracle
----
. 在 Oracle DB 中建立 PC 監控使用者 ID 。
+
....

[root@ip-172-30-15-111 ec2-user]# su - oracle
Last login: Fri Sep 13 18:12:24 UTC 2024 on pts/0
[oracle@ip-172-30-15-111 ~]$ sqlplus / as sysdba

SQL*Plus: Release 19.0.0.0.0 - Production on Fri Sep 13 19:08:41 2024
Version 19.18.0.0.0

Copyright (c) 1982, 2022, Oracle.  All rights reserved.


Connected to:
Oracle Database 19c Enterprise Edition Release 19.0.0.0.0 - Production
Version 19.18.0.0.0

SQL> CREATE USER c##ocfmon IDENTIFIED BY "XXXXXXXX";

User created.

SQL> grant connect to c##ocfmon;

Grant succeeded.

SQL> exit
Disconnected from Oracle Database 19c Enterprise Edition Release 19.0.0.0.0 - Production
Version 19.18.0.0.0

....
. 將資料庫新增至 Oracle 資源群組。
+
[source, cli]
----
pcs resource create ntap ocf:heartbeat:oracle sid='NTAP' home='/u01/app/oracle/product/19.0.0/NTAP' user='oracle' monuser='C##OCFMON' monpassword='XXXXXXXX' monprofile='DEFAULT' --group oracle
----
. 將資料庫接聽程式新增至 Oracle 資源群組。
+
[source, cli]
----
pcs resource create listener ocf:heartbeat:oralsnr sid='NTAP' listener='listener.ntap' --group=oracle
----
. 將 Oracle 資源群組中的所有資源位置限制、更新為主要節點做為慣用節點。
+
[source, cli]
----
pcs constraint location privip prefers ip-172-30-15-111.ec2.internal
pcs constraint location vip prefers ip-172-30-15-111.ec2.internal
pcs constraint location u01 prefers ip-172-30-15-111.ec2.internal
pcs constraint location u02 prefers ip-172-30-15-111.ec2.internal
pcs constraint location u03 prefers ip-172-30-15-111.ec2.internal
pcs constraint location ntap prefers ip-172-30-15-111.ec2.internal
pcs constraint location listener prefers ip-172-30-15-111.ec2.internal
----
+
....

[root@ip-172-30-15-111 ec2-user]# pcs constraint config
Location Constraints:
  Resource: listener
    Enabled on:
      Node: ip-172-30-15-111.ec2.internal (score:INFINITY)
  Resource: ntap
    Enabled on:
      Node: ip-172-30-15-111.ec2.internal (score:INFINITY)
  Resource: privip
    Enabled on:
      Node: ip-172-30-15-111.ec2.internal (score:INFINITY)
  Resource: u01
    Enabled on:
      Node: ip-172-30-15-111.ec2.internal (score:INFINITY)
  Resource: u02
    Enabled on:
      Node: ip-172-30-15-111.ec2.internal (score:INFINITY)
  Resource: u03
    Enabled on:
      Node: ip-172-30-15-111.ec2.internal (score:INFINITY)
  Resource: vip
    Enabled on:
      Node: ip-172-30-15-111.ec2.internal (score:INFINITY)
Ordering Constraints:
Colocation Constraints:
Ticket Constraints:

....
. 驗證 Oracle 資源組態。
+
[source, cli]
----
pcs status
----
+
....

[root@ip-172-30-15-111 ec2-user]# pcs status
Cluster name: ora_ec2nfsx
Cluster Summary:
  * Stack: corosync (Pacemaker is running)
  * Current DC: ip-172-30-15-111.ec2.internal (version 2.1.7-5.1.el8_10-0f7f88312) - partition with quorum
  * Last updated: Fri Sep 13 19:25:32 2024 on ip-172-30-15-111.ec2.internal
  * Last change:  Fri Sep 13 19:23:40 2024 by root via root on ip-172-30-15-111.ec2.internal
  * 2 nodes configured
  * 8 resource instances configured

Node List:
  * Online: [ ip-172-30-15-5.ec2.internal ip-172-30-15-111.ec2.internal ]

Full List of Resources:
  * clusterfence        (stonith:fence_aws):     Started ip-172-30-15-111.ec2.internal
  * Resource Group: oracle:
    * privip    (ocf::heartbeat:awsvip):         Started ip-172-30-15-111.ec2.internal
    * vip       (ocf::heartbeat:IPaddr2):        Started ip-172-30-15-111.ec2.internal
    * u01       (ocf::heartbeat:Filesystem):     Started ip-172-30-15-111.ec2.internal
    * u02       (ocf::heartbeat:Filesystem):     Started ip-172-30-15-111.ec2.internal
    * u03       (ocf::heartbeat:Filesystem):     Started ip-172-30-15-111.ec2.internal
    * ntap      (ocf::heartbeat:oracle):         Started ip-172-30-15-111.ec2.internal
    * listener  (ocf::heartbeat:oralsnr):        Started ip-172-30-15-111.ec2.internal

Daemon Status:
  corosync: active/enabled
  pacemaker: active/enabled
  pcsd: active/enabled


....


====


=== 部署後 HA 驗證

[%collapsible]
====
部署之後、務必執行一些測試與驗證、以確保 PC Oracle 資料庫容錯移轉叢集已正確設定、並能如預期般運作。測試驗證包括受管理的容錯移轉、模擬非預期的資源故障、以及叢集保護機制的還原。

. 手動觸發待命節點的隔離功能、以驗證節點隔離、並觀察待命節點是否已離線、並在逾時後重新開機。
+
[source, cli]
----
pcs stonith fence <standbynodename>
----
+
....

[root@ip-172-30-15-111 ec2-user]# pcs stonith fence ip-172-30-15-5.ec2.internal
Node: ip-172-30-15-5.ec2.internal fenced
[root@ip-172-30-15-111 ec2-user]# pcs status
Cluster name: ora_ec2nfsx
Cluster Summary:
  * Stack: corosync (Pacemaker is running)
  * Current DC: ip-172-30-15-111.ec2.internal (version 2.1.7-5.1.el8_10-0f7f88312) - partition with quorum
  * Last updated: Fri Sep 13 21:58:45 2024 on ip-172-30-15-111.ec2.internal
  * Last change:  Fri Sep 13 21:55:12 2024 by root via root on ip-172-30-15-111.ec2.internal
  * 2 nodes configured
  * 8 resource instances configured

Node List:
  * Online: [ ip-172-30-15-111.ec2.internal ]
  * OFFLINE: [ ip-172-30-15-5.ec2.internal ]

Full List of Resources:
  * clusterfence        (stonith:fence_aws):     Started ip-172-30-15-111.ec2.internal
  * Resource Group: oracle:
    * privip    (ocf::heartbeat:awsvip):         Started ip-172-30-15-111.ec2.internal
    * vip       (ocf::heartbeat:IPaddr2):        Started ip-172-30-15-111.ec2.internal
    * u01       (ocf::heartbeat:Filesystem):     Started ip-172-30-15-111.ec2.internal
    * u02       (ocf::heartbeat:Filesystem):     Started ip-172-30-15-111.ec2.internal
    * u03       (ocf::heartbeat:Filesystem):     Started ip-172-30-15-111.ec2.internal
    * ntap      (ocf::heartbeat:oracle):         Started ip-172-30-15-111.ec2.internal
    * listener  (ocf::heartbeat:oralsnr):        Started ip-172-30-15-111.ec2.internal

Daemon Status:
  corosync: active/enabled
  pacemaker: active/enabled
  pcsd: active/enabled


....
. 藉由終止接聽程式程序來模擬資料庫接聽程式失敗、並觀察電腦是否監控接聽程式失敗並在數秒內重新啟動。
+
....

[root@ip-172-30-15-111 ec2-user]# ps -ef | grep lsnr
oracle    154895       1  0 18:15 ?        00:00:00 /u01/app/oracle/product/19.0.0/NTAP/bin/tnslsnr listener.ntap -inherit
root      217779  120186  0 19:36 pts/0    00:00:00 grep --color=auto lsnr
[root@ip-172-30-15-111 ec2-user]# kill -9 154895

[root@ip-172-30-15-111 ec2-user]# su - oracle
Last login: Thu Sep 19 14:58:54 UTC 2024
[oracle@ip-172-30-15-111 ~]$ lsnrctl status listener.ntap

LSNRCTL for Linux: Version 19.0.0.0.0 - Production on 13-SEP-2024 19:36:51

Copyright (c) 1991, 2022, Oracle.  All rights reserved.

Connecting to (DESCRIPTION=(ADDRESS=(PROTOCOL=TCP)(HOST=172.30.15.33)(PORT=1521)))
TNS-12541: TNS:no listener
 TNS-12560: TNS:protocol adapter error
  TNS-00511: No listener
   Linux Error: 111: Connection refused
Connecting to (DESCRIPTION=(ADDRESS=(PROTOCOL=IPC)(KEY=EXTPROC1521)))
TNS-12541: TNS:no listener
 TNS-12560: TNS:protocol adapter error
  TNS-00511: No listener
   Linux Error: 111: Connection refused

[oracle@ip-172-30-15-111 ~]$ lsnrctl status listener.ntap

LSNRCTL for Linux: Version 19.0.0.0.0 - Production on 19-SEP-2024 15:00:10

Copyright (c) 1991, 2022, Oracle.  All rights reserved.

Connecting to (DESCRIPTION=(ADDRESS=(PROTOCOL=TCP)(HOST=172.30.15.33)(PORT=1521)))
STATUS of the LISTENER
------------------------
Alias                     listener.ntap
Version                   TNSLSNR for Linux: Version 19.0.0.0.0 - Production
Start Date                16-SEP-2024 14:00:14
Uptime                    3 days 0 hr. 59 min. 56 sec
Trace Level               off
Security                  ON: Local OS Authentication
SNMP                      OFF
Listener Parameter File   /u01/app/oracle/product/19.0.0/NTAP/network/admin/listener.ora
Listener Log File         /u01/app/oracle/diag/tnslsnr/ip-172-30-15-111/listener.ntap/alert/log.xml
Listening Endpoints Summary...
  (DESCRIPTION=(ADDRESS=(PROTOCOL=tcp)(HOST=172.30.15.33)(PORT=1521)))
  (DESCRIPTION=(ADDRESS=(PROTOCOL=ipc)(KEY=EXTPROC1521)))
  (DESCRIPTION=(ADDRESS=(PROTOCOL=tcps)(HOST=ip-172-30-15-111.ec2.internal)(PORT=5500))(Security=(my_wallet_directory=/u01/app/oracle/product/19.0.0/NTAP/admin/NTAP/xdb_wallet))(Presentation=HTTP)(Session=RAW))
Services Summary...
Service "21f0b5cc1fa290e2e0636f0f1eacfd43.ec2.internal" has 1 instance(s).
  Instance "NTAP", status READY, has 1 handler(s) for this service...
Service "21f0b74445329119e0636f0f1eacec03.ec2.internal" has 1 instance(s).
  Instance "NTAP", status READY, has 1 handler(s) for this service...
Service "21f0b83929709164e0636f0f1eacacc3.ec2.internal" has 1 instance(s).
  Instance "NTAP", status READY, has 1 handler(s) for this service...
Service "NTAP.ec2.internal" has 1 instance(s).
  Instance "NTAP", status READY, has 1 handler(s) for this service...
Service "NTAPXDB.ec2.internal" has 1 instance(s).
  Instance "NTAP", status READY, has 1 handler(s) for this service...
Service "ntap_pdb1.ec2.internal" has 1 instance(s).
  Instance "NTAP", status READY, has 1 handler(s) for this service...
Service "ntap_pdb2.ec2.internal" has 1 instance(s).
  Instance "NTAP", status READY, has 1 handler(s) for this service...
Service "ntap_pdb3.ec2.internal" has 1 instance(s).
  Instance "NTAP", status READY, has 1 handler(s) for this service...
The command completed successfully

....
. 模擬資料庫故障、方法是終止 Pmon 程序、觀察電腦是否監控資料庫故障、並在數秒內重新啟動。
+
....

**Make a remote connection to ntap database**

[oracle@ora_01 ~]$ sqlplus system@//172.30.15.33:1521/NTAP.ec2.internal

SQL*Plus: Release 19.0.0.0.0 - Production on Fri Sep 13 15:42:42 2024
Version 19.18.0.0.0

Copyright (c) 1982, 2022, Oracle.  All rights reserved.

Enter password:
Last Successful login time: Thu Sep 12 2024 13:37:28 -04:00

Connected to:
Oracle Database 19c Enterprise Edition Release 19.0.0.0.0 - Production
Version 19.18.0.0.0

SQL> select instance_name, host_name from v$instance;

INSTANCE_NAME
----------------
HOST_NAME
----------------------------------------------------------------
NTAP
ip-172-30-15-111.ec2.internal


SQL>

**Kill ntap pmon process to simulate a failure**

[root@ip-172-30-15-111 ec2-user]# ps -ef | grep pmon
oracle    159247       1  0 18:27 ?        00:00:00 ora_pmon_NTAP
root      230595  120186  0 19:44 pts/0    00:00:00 grep --color=auto pmon
[root@ip-172-30-15-111 ec2-user]# kill -9 159247

**Observe the DB failure**

SQL> /
select instance_name, host_name from v$instance
*
ERROR at line 1:
ORA-03113: end-of-file on communication channel
Process ID: 227424
Session ID: 396 Serial number: 4913


SQL> exit
Disconnected from Oracle Database 19c Enterprise Edition Release 19.0.0.0.0 - Production
Version 19.18.0.0.0

**Reconnect to DB after reboot**

[oracle@ora_01 ~]$ sqlplus system@//172.30.15.33:1521/NTAP.ec2.internal

SQL*Plus: Release 19.0.0.0.0 - Production on Fri Sep 13 15:47:24 2024
Version 19.18.0.0.0

Copyright (c) 1982, 2022, Oracle.  All rights reserved.

Enter password:
Last Successful login time: Fri Sep 13 2024 15:42:47 -04:00

Connected to:
Oracle Database 19c Enterprise Edition Release 19.0.0.0.0 - Production
Version 19.18.0.0.0

SQL> select instance_name, host_name from v$instance;

INSTANCE_NAME
----------------
HOST_NAME
----------------------------------------------------------------
NTAP
ip-172-30-15-111.ec2.internal


SQL>


....
. 將主節點設為待命模式、將 Oracle 資源容錯移轉至待命節點、以驗證受管理資料庫從主要節點容錯移轉至待命節點。
+
[source, cli]
----
pcs node standby <nodename>
----
+
....

**Stopping Oracle resources on primary node in reverse order**

[root@ip-172-30-15-111 ec2-user]# pcs node standby ip-172-30-15-111.ec2.internal
[root@ip-172-30-15-111 ec2-user]# pcs status
Cluster name: ora_ec2nfsx
Cluster Summary:
  * Stack: corosync (Pacemaker is running)
  * Current DC: ip-172-30-15-111.ec2.internal (version 2.1.7-5.1.el8_10-0f7f88312) - partition with quorum
  * Last updated: Fri Sep 13 20:01:16 2024 on ip-172-30-15-111.ec2.internal
  * Last change:  Fri Sep 13 20:01:08 2024 by root via root on ip-172-30-15-111.ec2.internal
  * 2 nodes configured
  * 8 resource instances configured

Node List:
  * Node ip-172-30-15-111.ec2.internal: standby (with active resources)
  * Online: [ ip-172-30-15-5.ec2.internal ]

Full List of Resources:
  * clusterfence        (stonith:fence_aws):     Started ip-172-30-15-5.ec2.internal
  * Resource Group: oracle:
    * privip    (ocf::heartbeat:awsvip):         Started ip-172-30-15-111.ec2.internal
    * vip       (ocf::heartbeat:IPaddr2):        Started ip-172-30-15-111.ec2.internal
    * u01       (ocf::heartbeat:Filesystem):     Stopping ip-172-30-15-111.ec2.internal
    * u02       (ocf::heartbeat:Filesystem):     Stopped
    * u03       (ocf::heartbeat:Filesystem):     Stopped
    * ntap      (ocf::heartbeat:oracle):         Stopped
    * listener  (ocf::heartbeat:oralsnr):        Stopped

Daemon Status:
  corosync: active/enabled
  pacemaker: active/enabled
  pcsd: active/enabled

**Starting Oracle resources on standby node in sequencial order**

[root@ip-172-30-15-111 ec2-user]# pcs status
Cluster name: ora_ec2nfsx
Cluster Summary:
  * Stack: corosync (Pacemaker is running)
  * Current DC: ip-172-30-15-111.ec2.internal (version 2.1.7-5.1.el8_10-0f7f88312) - partition with quorum
  * Last updated: Fri Sep 13 20:01:34 2024 on ip-172-30-15-111.ec2.internal
  * Last change:  Fri Sep 13 20:01:08 2024 by root via root on ip-172-30-15-111.ec2.internal
  * 2 nodes configured
  * 8 resource instances configured

Node List:
  * Node ip-172-30-15-111.ec2.internal: standby
  * Online: [ ip-172-30-15-5.ec2.internal ]

Full List of Resources:
  * clusterfence        (stonith:fence_aws):     Started ip-172-30-15-5.ec2.internal
  * Resource Group: oracle:
    * privip    (ocf::heartbeat:awsvip):         Started ip-172-30-15-5.ec2.internal
    * vip       (ocf::heartbeat:IPaddr2):        Started ip-172-30-15-5.ec2.internal
    * u01       (ocf::heartbeat:Filesystem):     Started ip-172-30-15-5.ec2.internal
    * u02       (ocf::heartbeat:Filesystem):     Started ip-172-30-15-5.ec2.internal
    * u03       (ocf::heartbeat:Filesystem):     Started ip-172-30-15-5.ec2.internal
    * ntap      (ocf::heartbeat:oracle):         Starting ip-172-30-15-5.ec2.internal
    * listener  (ocf::heartbeat:oralsnr):        Stopped

Daemon Status:
  corosync: active/enabled
  pacemaker: active/enabled
  pcsd: active/enabled

**NFS mount points mounted on standby node**

[root@ip-172-30-15-5 ec2-user]# df -h
Filesystem                 Size  Used Avail Use% Mounted on
devtmpfs                   7.7G     0  7.7G   0% /dev
tmpfs                      7.7G   33M  7.7G   1% /dev/shm
tmpfs                      7.7G   17M  7.7G   1% /run
tmpfs                      7.7G     0  7.7G   0% /sys/fs/cgroup
/dev/xvda2                  50G   21G   30G  41% /
tmpfs                      1.6G     0  1.6G   0% /run/user/1000
172.30.15.95:/orapm01_u01   48T   47T  840G  99% /u01
172.30.15.95:/orapm01_u02  285T  285T  840G 100% /u02
172.30.15.95:/orapm01_u03  190T  190T  840G 100% /u03
tmpfs                      1.6G     0  1.6G   0% /run/user/54321

**Database opened on standby node**

[oracle@ora_01 ~]$ sqlplus system@//172.30.15.33:1521/NTAP.ec2.internal

SQL*Plus: Release 19.0.0.0.0 - Production on Fri Sep 13 16:34:08 2024
Version 19.18.0.0.0

Copyright (c) 1982, 2022, Oracle.  All rights reserved.

Enter password:
Last Successful login time: Fri Sep 13 2024 15:47:28 -04:00

Connected to:
Oracle Database 19c Enterprise Edition Release 19.0.0.0.0 - Production
Version 19.18.0.0.0

SQL> select name, open_mode from v$database;

NAME      OPEN_MODE
--------- --------------------
NTAP      READ WRITE

SQL> select instance_name, host_name from v$instance;

INSTANCE_NAME
----------------
HOST_NAME
----------------------------------------------------------------
NTAP
ip-172-30-15-5.ec2.internal


SQL>

....
. 透過非待命主要節點、驗證受管理資料庫的容錯回復、並觀察 Oracle 資源是否因偏好的節點設定而自動容錯回復。
+
[source, cli]
----
pcs node unstandby <nodename>
----
+
....
**Stopping Oracle resources on standby node for failback to primary**

[root@ip-172-30-15-111 ec2-user]# pcs node unstandby ip-172-30-15-111.ec2.internal
[root@ip-172-30-15-111 ec2-user]# pcs status
Cluster name: ora_ec2nfsx
Cluster Summary:
  * Stack: corosync (Pacemaker is running)
  * Current DC: ip-172-30-15-111.ec2.internal (version 2.1.7-5.1.el8_10-0f7f88312) - partition with quorum
  * Last updated: Fri Sep 13 20:41:30 2024 on ip-172-30-15-111.ec2.internal
  * Last change:  Fri Sep 13 20:41:18 2024 by root via root on ip-172-30-15-111.ec2.internal
  * 2 nodes configured
  * 8 resource instances configured

Node List:
  * Online: [ ip-172-30-15-5.ec2.internal ip-172-30-15-111.ec2.internal ]

Full List of Resources:
  * clusterfence        (stonith:fence_aws):     Started ip-172-30-15-5.ec2.internal
  * Resource Group: oracle:
    * privip    (ocf::heartbeat:awsvip):         Stopping ip-172-30-15-5.ec2.internal
    * vip       (ocf::heartbeat:IPaddr2):        Stopped
    * u01       (ocf::heartbeat:Filesystem):     Stopped
    * u02       (ocf::heartbeat:Filesystem):     Stopped
    * u03       (ocf::heartbeat:Filesystem):     Stopped
    * ntap      (ocf::heartbeat:oracle):         Stopped
    * listener  (ocf::heartbeat:oralsnr):        Stopped

Daemon Status:
  corosync: active/enabled
  pacemaker: active/enabled
  pcsd: active/enabled

**Starting Oracle resources on primary node for failback**

[root@ip-172-30-15-111 ec2-user]# pcs status
Cluster name: ora_ec2nfsx
Cluster Summary:
  * Stack: corosync (Pacemaker is running)
  * Current DC: ip-172-30-15-111.ec2.internal (version 2.1.7-5.1.el8_10-0f7f88312) - partition with quorum
  * Last updated: Fri Sep 13 20:41:45 2024 on ip-172-30-15-111.ec2.internal
  * Last change:  Fri Sep 13 20:41:18 2024 by root via root on ip-172-30-15-111.ec2.internal
  * 2 nodes configured
  * 8 resource instances configured

Node List:
  * Online: [ ip-172-30-15-5.ec2.internal ip-172-30-15-111.ec2.internal ]

Full List of Resources:
  * clusterfence        (stonith:fence_aws):     Started ip-172-30-15-5.ec2.internal
  * Resource Group: oracle:
    * privip    (ocf::heartbeat:awsvip):         Started ip-172-30-15-111.ec2.internal
    * vip       (ocf::heartbeat:IPaddr2):        Started ip-172-30-15-111.ec2.internal
    * u01       (ocf::heartbeat:Filesystem):     Started ip-172-30-15-111.ec2.internal
    * u02       (ocf::heartbeat:Filesystem):     Started ip-172-30-15-111.ec2.internal
    * u03       (ocf::heartbeat:Filesystem):     Started ip-172-30-15-111.ec2.internal
    * ntap      (ocf::heartbeat:oracle):         Starting ip-172-30-15-111.ec2.internal
    * listener  (ocf::heartbeat:oralsnr):        Stopped

Daemon Status:
  corosync: active/enabled
  pacemaker: active/enabled
  pcsd: active/enabled

**Database now accepts connection on primary node**

[oracle@ora_01 ~]$ sqlplus system@//172.30.15.33:1521/NTAP.ec2.internal

SQL*Plus: Release 19.0.0.0.0 - Production on Fri Sep 13 16:46:07 2024
Version 19.18.0.0.0

Copyright (c) 1982, 2022, Oracle.  All rights reserved.

Enter password:
Last Successful login time: Fri Sep 13 2024 16:34:12 -04:00

Connected to:
Oracle Database 19c Enterprise Edition Release 19.0.0.0.0 - Production
Version 19.18.0.0.0

SQL> select instance_name, host_name from v$instance;

INSTANCE_NAME
----------------
HOST_NAME
----------------------------------------------------------------
NTAP
ip-172-30-15-111.ec2.internal


SQL>

....


這將完成 AWS EC2 中的 Oracle HA 驗證與解決方案示範、並將 Pacemaker 叢集和 Amazon FSX ONTAP 做為資料庫儲存後端。

====


=== 使用 SnapCenter 進行 Oracle 備份、還原及複製

[%collapsible]
====
NetApp 建議使用 SnapCenter UI 工具來管理部署在 AWS EC2 和 Amazon FSX ONTAP 中的 Oracle 資料庫。link:aws_ora_fsx_vmc_guestmount.html#oracle-backup-restore-and-clone-with-snapcenter["在 VMware Cloud 上使用來賓安裝的 FSX ONTAP 、在 AWS 上使用簡化的自我管理 Oracle"^] `Oracle backup, restore, and clone with SnapCenter`如需設定 SnapCenter 及執行資料庫備份、還原及複製工作流程的詳細資訊、請參閱 TR-4979 。

====


== 何處可找到其他資訊

若要深入瞭解本文件所述資訊、請參閱下列文件及 / 或網站：

* 設定及管理高可用度叢集
+
link:https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/8/html/configuring_and_managing_high_availability_clusters/index["https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/8/html/configuring_and_managing_high_availability_clusters/index"^]

* NetApp企業資料庫解決方案
+
link:https://docs.netapp.com/us-en/netapp-solutions/databases/index.html["https://docs.netapp.com/us-en/netapp-solutions/databases/index.html"^]

* Amazon FSX for NetApp ONTAP 產品
+
link:https://aws.amazon.com/fsx/netapp-ontap/?refid=3c5ce89c-8865-47a3-bec3-f6820351aa6d["https://aws.amazon.com/fsx/netapp-ontap/?refid=3c5ce89c-8865-47a3-bec3-f6820351aa6d"^]

* 部署 Oracle Direct NFS
+
link:https://docs.oracle.com/en/database/oracle/oracle-database/19/ladbi/deploying-dnfs.html#GUID-D06079DB-8C71-4F68-A1E3-A75D7D96DCE2["https://docs.oracle.com/en/database/oracle/oracle-database/19/ladbi/deploying-dnfs.html#GUID-D06079DB-8C71-4F68-A1E3-A75D7D96DCE2"^]


