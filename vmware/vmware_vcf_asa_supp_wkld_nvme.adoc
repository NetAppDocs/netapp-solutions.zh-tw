---
sidebar: sidebar 
permalink: vmware/vmware_vcf_asa_supp_wkld_nvme.html 
keywords: netapp, vmware, cloud, foundation, vcf, aff, all-flash, nfs, vvol, vvols, array, ontap tools, otv, sddc, iscsi 
summary:  
---
= 為 VCF 工作負載網域設定 NVMe / TCP 補充儲存設備
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./../media/


[role="lead"]
在此案例中、我們將示範如何為 VCF 工作負載網域設定 NVMe / TCP 補充儲存設備。

作者： Josh Powell



== 為 VCF 工作負載網域設定 NVMe / TCP 補充儲存設備



=== 案例總覽

此案例涵蓋下列高層級步驟：

* 為 NVMe / TCP 流量建立具有邏輯介面（生命）的儲存虛擬機器（ SVM ）。
* 在 VI 工作負載網域上為 iSCSI 網路建立分散式連接埠群組。
* 在 ESXi 主機上為 VI 工作負載網域建立 iSCSI 的 vmkernel 介面卡。
* 在 ESXi 主機上新增 NVMe / TCP 介面卡。
* 部署 NVMe / TCP 資料存放區。




=== 先決條件

此案例需要下列元件和組態：

* ONTAP ASA 儲存系統、乙太網路交換器上的實體資料連接埠專用於儲存流量。
* vcf 管理網域部署已完成、 vSphere 用戶端可存取。
* 先前已部署 VI 工作負載網域。


NetApp 建議使用 NVMe / TCP 的完整備援網路設計。下圖說明備援組態的範例、為儲存系統、交換器、網路卡和主機系統提供容錯能力。請參閱 NetApp link:https://docs.netapp.com/us-en/ontap/san-config/index.html["SAN組態參考"] 以取得更多資訊。

image:vmware-vcf-asa-image74.png["NVMe TCP 網路設計"]

對於跨多個路徑的多重路徑和容錯移轉、 NetApp 建議在個別乙太網路中、每個儲存節點至少有兩個生命期、用於 NVMe / TCP 組態中的所有 SVM 。

本文件示範建立新 SVM 的程序、並指定 IP 位址資訊、以建立多個 NVMe / TCP 流量的生命週期。若要新增生命至現有 SVM 、請參閱 link:https://docs.netapp.com/us-en/ontap/networking/create_a_lif.html["建立 LIF （網路介面）"]。

如需 ONTAP 儲存系統 NVMe 設計考量的其他資訊、請參閱 link:https://docs.netapp.com/us-en/ontap/nvme/support-limitations.html["NVMe 組態、支援和限制"]。



=== 部署步驟

若要使用 NVMe / TCP 在 VCF 工作負載網域上建立 VMFS 資料存放區、請完成下列步驟。



==== 在 ONTAP 儲存系統上建立 SVM 、 Lifs 和 NVMe 命名空間

下列步驟是在 ONTAP 系統管理員中執行。

.建立儲存 VM 和生命
[%collapsible]
====
請完成下列步驟、為 NVMe / TCP 流量建立 SVM 及多個生命。

. 從 ONTAP 系統管理員瀏覽至左側功能表中的 * 儲存 VM* 、然後按一下 *+ Add* 開始。
+
image:vmware-vcf-asa-image01.png["按一下 + 新增以開始建立 SVM"]

+
｛ nbsp ｝

. 在 * 新增儲存虛擬機器 * 精靈中、為 SVM 提供 * 名稱 * 、選取 * IP 空間 * 、然後在 * 存取傳輸協定 * 下、按一下 *NVMe * 標籤、並勾選 * 啟用 NVMe / TCP * 方塊。
+
image:vmware-vcf-asa-image75.png["新增儲存 VM 精靈 - 啟用 NVMe / TCP"]

+
｛ nbsp ｝

. 在 * 網路介面 * 區段中、填寫第一個 LIF 的 * IP 位址 * 、 * 子網路遮罩 * 和 * 廣播網域和連接埠 * 。對於後續的生命、核取方塊可以啟用、以便在所有剩餘的生命中使用一般設定、或使用個別的設定。
+

NOTE: 對於跨多個路徑的多重路徑和容錯移轉、 NetApp 建議在個別的乙太網路中、每個儲存節點至少有兩個生命期、用於 NVMe / TCP 組態中的所有 SVM 。

+
image:vmware-vcf-asa-image76.png["填寫網路資訊以取得生命"]

+
｛ nbsp ｝

. 選擇是否啟用 Storage VM Administration 帳戶（適用於多租戶環境）、然後按一下 * Save* 以建立 SVM 。
+
image:vmware-vcf-asa-image04.png["啟用 SVM 帳戶並完成"]



====
.建立 NVMe 命名空間
[%collapsible]
====
NVMe 命名空間類似於 iSCSI 或 FC 的 LUN 。必須先建立 NVMe 命名空間、才能從 vSphere Client 部署 VMFS 資料存放區。若要建立 NVMe 命名空間、必須先從叢集中的每個 ESXi 主機取得 NVMe 合格名稱（ NQN ）。NQN 是由 ONTAP 用來提供命名空間的存取控制。

完成下列步驟以建立 NVMe 命名空間：

. 開啟與叢集中 ESXi 主機的 SSH 工作階段、以取得其 NQN 。從 CLI 使用下列命令：
+
[source, cli]
----
esxcli nvme info get
----
+
應顯示類似下列內容的輸出：

+
[source, cli]
----
Host NQN: nqn.2014-08.com.netapp.sddc:nvme:vcf-wkld-esx01
----
. 記錄叢集中每個 ESXi 主機的 NQN
. 從 ONTAP 系統管理員瀏覽至左側功能表中的 * NVMe 命名空間 * 、然後按一下「 *+ Add* 」開始。
+
image:vmware-vcf-asa-image93.png["按一下「 + 新增」以建立 NVMe 命名空間"]

+
｛ nbsp ｝

. 在「 * 新增 NVMe 命名空間 * 」頁面上、填入名稱首碼、要建立的命名空間數目、命名空間的大小、以及要存取命名空間的主機作業系統。在 * 主機 NQN* 區段中、建立一個以逗號分隔的 NQN 清單、列出先前從將存取命名空間的 ESXi 主機收集的 NQN 。


按一下「 * 更多選項 * 」以設定其他項目、例如快照保護原則。最後、按一下 * 儲存 * 來建立 NVMe 命名空間。

+
image:vmware-vcf-asa-image93.png["按一下「 + 新增」以建立 NVMe 命名空間"]

====


==== 在 ESXi 主機上設定網路和 NVMe 軟體介面卡

下列步驟是使用 vSphere 用戶端在 VI 工作負載網域叢集上執行。在此案例中、我們使用 vCenter 單一登入、因此 vSphere 用戶端在管理和工作負載網域中都是通用的。

.為 NVMe / TCP 流量建立分散式連接埠群組
[%collapsible]
====
完成下列步驟、為每個 NVMe / TCP 網路建立新的分散式連接埠群組：

. 從 vSphere 用戶端瀏覽至工作負載網域的 * 清查 > 網路 * 。瀏覽至現有的分散式交換器、然後選擇建立 * 新的分散式連接埠群組 ... * 的動作。
+
image:vmware-vcf-asa-image22.png["選擇以建立新的連接埠群組"]

+
｛ nbsp ｝

. 在 * 新增分散式連接埠群組 * 精靈中、填入新連接埠群組的名稱、然後按一下 * 下一步 * 繼續。
. 在「 * 組態設定 * 」頁面上、填寫所有設定。如果使用 VLAN 、請務必提供正確的 VLAN ID 。按一下 * 下一步 * 繼續。
+
image:vmware-vcf-asa-image23.png["填寫 VLAN ID"]

+
｛ nbsp ｝

. 在「 * 準備完成 * 」頁面上、檢閱變更、然後按一下「 * 完成 * 」來建立新的分散式連接埠群組。
. 重複此程序、為第二個使用的 NVMe / TCP 網路建立分散式連接埠群組、並確保您輸入正確的 * VLAN ID* 。
. 建立兩個連接埠群組之後、請瀏覽至第一個連接埠群組、然後選取「 * 編輯設定 ... * 」動作。
+
image:vmware-vcf-asa-image77.png["DPG - 編輯設定"]

+
｛ nbsp ｝

. 在 * 分散式連接埠群組 - 編輯設定 * 頁面上、瀏覽左側功能表中的 * 成組和容錯移轉 * 、然後按一下 * 上線 2* 將其向下移至 * 未使用的上行鏈路 * 。
+
image:vmware-vcf-asa-image78.png["將 uplink2 移至未使用的"]

. 對第二個 NVMe / TCP 連接埠群組重複此步驟。但是，這次將 *uplink1* 向下移到 * 未使用的上行鏈路 * 。
+
image:vmware-vcf-asa-image79.png["將上行鏈路 1 移至未使用的"]



====
.在每個 ESXi 主機上建立 VMkernel 介面卡
[%collapsible]
====
在工作負載網域中的每個 ESXi 主機上重複此程序。

. 從 vSphere 用戶端導覽至工作負載網域清查中的其中一個 ESXi 主機。從 * 組態 * 標籤中選取 * VMkernel 介面卡 * 、然後按一下 * 新增網路 ... * 開始。
+
image:vmware-vcf-asa-image30.png["開始新增網路精靈"]

+
｛ nbsp ｝

. 在 *Select connection type* （選擇連接類型 * ）窗口中選擇 *VMkernel Network Adapter* （ VMkernel 網絡適配器 * ），然後單擊 *Next* （下一步）繼續。
+
image:vmware-vcf-asa-image08.png["選擇 [VMkernel 網路介面卡 ]"]

+
｛ nbsp ｝

. 在 * 選取目標裝置 * 頁面上、選擇先前建立的 iSCSI 分散式連接埠群組之一。
+
image:vmware-vcf-asa-image95.png["選擇目標連接埠群組"]

+
｛ nbsp ｝

. 在「 * 連接埠內容 * 」頁面上、按一下「 *NVMe over TCP* 」方塊、然後按一下「 * 下一步 * 」繼續。
+
image:vmware-vcf-asa-image96.png["VMkernel 連接埠內容"]

+
｛ nbsp ｝

. 在 *IPv4 settings* 頁面上，填寫 *IP 地址 * 、 * 子網掩碼 * ，並提供新的網關 IP 地址（僅在需要時）。按一下 * 下一步 * 繼續。
+
image:vmware-vcf-asa-image97.png["VMkernel IPv4 設定"]

+
｛ nbsp ｝

. 在「 * 準備完成 * 」頁面上檢閱您的選擇、然後按一下「 * 完成 * 」來建立 VMkernel 介面卡。
+
image:vmware-vcf-asa-image98.png["檢閱 VMkernel 選擇"]

+
｛ nbsp ｝

. 重複此程序、為第二個 iSCSI 網路建立 VMkernel 介面卡。


====
.透過 TCP 介面卡新增 NVMe
[%collapsible]
====
工作負載網域叢集中的每個 ESXi 主機都必須為每個專為儲存流量而建立的 NVMe / TCP 網路安裝 NVMe over TCP 軟體介面卡。

若要透過 TCP 介面卡安裝 NVMe 並探索 NVMe 控制器、請完成下列步驟：

. 在 vSphere 用戶端中、導覽至工作負載網域叢集中的其中一個 ESXi 主機。從 * 組態 * 標籤按一下功能表中的 * 儲存介面卡 * 、然後從 * 新增軟體介面卡 * 下拉式功能表中、選取 * 透過 TCP 介面卡 * 新增 NVMe 。
+
image:vmware-vcf-asa-image99.png["透過 TCP 介面卡新增 NVMe"]

+
｛ nbsp ｝

. 在 * 透過 TCP 介面卡 * 新增軟體 NVMe 視窗中、存取 * 實體網路介面卡 * 下拉式功能表、並選取正確的實體網路介面卡、以啟用 NVMe 介面卡。
+
image:vmware-vcf-asa-image100.png["選取實體介面卡"]

+
｛ nbsp ｝

. 針對第二個指派給 NVMe over TCP 流量的網路重複此程序、指派正確的實體介面卡。
. 選取其中一個新安裝的 NVMe over TCP 介面卡、然後在 * 控制器 * 索引標籤上選取 * 新增控制器 * 。
+
image:vmware-vcf-asa-image101.png["新增控制器"]

+
｛ nbsp ｝

. 在 * 新增控制器 * 視窗中、選取 * 自動 * 標籤、然後完成下列步驟。
+
** 在指派給此 NVMe over TCP 介面卡的實體介面卡所在的相同網路上、為其中一個 SVM 邏輯介面填寫 IP 位址。
** 按一下 * 探索控制器 * 按鈕。
** 從探索到的控制器清單中、按一下網路位址與此 NVMe over TCP 介面卡對齊的兩個控制器核取方塊。
** 按一下 * 確定 * 按鈕以新增選取的控制器。
+
image:vmware-vcf-asa-image102.png["探索及新增控制器"]

+
｛ nbsp ｝



. 幾秒鐘後、您應該會在「裝置」標籤上看到 NVMe 命名空間。
+
image:vmware-vcf-asa-image103.png["NVMe 命名空間列在「裝置」下"]

+
｛ nbsp ｝

. 重複此程序、為針對 NVMe / TCP 流量建立的第二個網路建立 NVMe over TCP 介面卡。


====
.透過 TCP 資料存放區部署 NVMe
[%collapsible]
====
若要在 NVMe 命名空間上建立 VMFS 資料存放區、請完成下列步驟：

. 在 vSphere 用戶端中、導覽至工作負載網域叢集中的其中一個 ESXi 主機。從 * 「動作」 * 功能表中選取 * 「儲存」 > 「新資料存放區 ... 」 * 。
+
image:vmware-vcf-asa-image104.png["透過 TCP 介面卡新增 NVMe"]

+
｛ nbsp ｝

. 在 * 新資料存放區 * 精靈中、選取 * VMS* 作為類型。按一下 * 下一步 * 繼續。
. 在 * 名稱和裝置選擇 * 頁面上、提供資料存放區的名稱、然後從可用裝置清單中選取 NVMe 命名空間。
+
image:vmware-vcf-asa-image105.png["名稱與裝置選擇"]

+
｛ nbsp ｝

. 在 *VMFS 版本 * 頁面上、選取資料存放區的 VMFS 版本。
. 在「 * 分割區組態 * 」頁面上、對預設分割區配置進行任何所需的變更。按一下 * 下一步 * 繼續。
+
image:vmware-vcf-asa-image106.png["NVMe 分割區組態"]

+
｛ nbsp ｝

. 在 * 準備完成 * 頁面上、檢閱摘要、然後按一下 * 完成 * 來建立資料存放區。
. 瀏覽至庫存中的新資料存放區、然後按一下 * 主機 * 索引標籤。如果設定正確、則叢集中的所有 ESXi 主機都應列出、並可存取新的資料存放區。
+
image:vmware-vcf-asa-image107.png["連線至資料存放區的主機"]

+
｛ nbsp ｝



====


=== 其他資訊

如需設定 ONTAP 儲存系統的相關資訊、請參閱 link:https://docs.netapp.com/us-en/ontap["供應說明文件ONTAP"] 中心。

如需設定 VCF 的詳細資訊、請參閱 link:https://docs.vmware.com/en/VMware-Cloud-Foundation/index.html["VMware Cloud Foundation 文件"]。
